<!DOCTYPE html>
<html>
  <head>
    <title>NLP算法</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  </head>
  <body>

    <h1>NLP算法</h1>

    <p>陈一帅</p>

    <p><a href="mailto:yschen@bjtu.edu.cn">yschen@bjtu.edu.cn</a></p>

    <p>北京交通大学电子信息工程学院网络智能实验室</p>

    <p>
      北京交通大学《NLP算法》课程，源自里斯本机器学习夏令营材料，讲解自然语言信息处理和应用系统设计的基本原理和算法，一路下来，带大家在动手中，走上算法研发的职业道路。详细课程信息请访问：https://yishuai.github.io/bigalgo/nlp-algo.html
    </p>

    <h3>目录</h3>
    <ol>
    <li><a href="#algo-math">数学基础</a></li>
    <li><a href="#algo-opti">优化</a></li>
    <li><a href="#algo-classify">文本特征和分类</a></li>
    <li><a href="#hmm">序列文本学习(HMM)</a></li>
    <li><a href="#crf">结构化预测(CRF)</a></li>
    <li><a href="#nntxt">深度文本处理技术</a></li>
    </ol>

    <p> LxMLS 2021 夏令营材料</p>
    <ol>
      <li>夏令营：<a href="http://lxmls.it.pt/2021/?page_id=79">主页</a></li>
      <li>课程视频：<a href="https://www.youtube.com/c/LxMLSLisbonMachineLearningSchool/">Youtube</a></li>
      <li>实验手册：<a href="http://lxmls.it.pt/2021/LxMLS_guide_2021.pdf">PDF</a></li>
      <li>实验代码：<a href="https://github.com/LxMLS/lxmls-toolkit">Github</a></li>
      <li>重点提示：<a href="ppt/17-text/lisben-index.pdf">PDF</a></li>
    </ol>

    <p>说明</p>
    <ol>
      <li>将代码和模型相印证，是学习机器学习和深度学习的最佳方法</li>
      <li>结合上课内容，搞懂模型是怎么代码实现的，直到自己能写出来，才真正理解和掌握了这些算法，并具备了机器学习的研究和开发能力</li>
      <li>这套代码就是一套帮助你掌握这些算法的最佳工具</li>
      <li>这些代码非常清晰，易懂。某种程度上，比看公式推导，更容易懂</li>
      <li>目标是自己能从0开始，把算法写出来，并把实验完成</li>
      <li>这是真正值得学习的</li>
    </ol>

    <h3><a name="algo-math">1、数学基础</a></h3>

    <p>
      本节介绍大数据算法的数学基础
    </p>
      <p>材料</p>
      <ol>
        <li>Mario Figueiredo，概率论和线性代数基础教程，PDF，<a
            href="http://lxmls.it.pt/2021/wp-content/uploads/2021/07/LxMLS_Lecture_0_handout.pdf">原链接</a>, <a
            href="ppt/17-text/0_math.pdf">本地链接</a></li>
      </ol>
      <p>重点</p>
      <ol>
        <li>Binomial分布，Multinomial分布</li>
        <li>高斯分布</li>
        <li>贝叶斯定理</li>
        <li>指数族：重点的重点。第36页-第38页详细推导</li>
        <li>特别是“指数家族”概率分布的Partition函数对\eta求导过程。这对理解后面的所有模型（直到CRF）都非常关键</li>
        <li>向量空间、范数、内积</li>
        <li>高斯-施瓦茨不等式，特别是第59页其精妙的证明</li>
        <li>矩阵的Rank、SVD</li>
        <li>Convex函数</li>
      </ol>

  <h3><a name="algo-opti">2、优化</a></h3>

  <p>
    本节介绍大数据算法的编程基础，包括Python入门、梯度下降、线性回归
  </p>
  <p>材料</p>
  <ol>
    <li>Luis Pedro Coelho, Python、Numpy、Matplotlib 入门，<a
        href="https://github.com/luispedro/talk-python-intro">Github</a></li>
    <li>Andre Martins，文本机器学习简介：线性学习器，PDF，<a
        href="http://lxmls.it.pt/2021/wp-content/uploads/2021/07/lxmls2021-andre-martins.pdf">原链接</a>, <a
        href="ppt/17-text/0_math.pdf">本地链接</a></li>
  </ol>
  <p>重点</p>
  <ol>
    <li>第26-34页，线性回归模型</li>
    <ol>
      <li>扩展，支持非线性</li>
      <li>误差函数概念：Squared Loss</li>
      <li>概率理解：基于高斯噪声和最大似然估计，基于高斯先验的L2正则</li>
    </ol>
    <li>第92页-93页，梯度下降优化方法</li>
  </ol>
  <p>实验：Lab5：父子身高相关吗？<a href="https://docs.qq.com/doc/DT1hkcUZZR2RvVktG">腾讯文档</a></p>

  <h3><a name="algo-classify">3、文本特征和分类</a></h3>

  <p>
    本节介绍大数据文本处理的特征工程和分类模型基本概念。
  </p>
  <p>材料：Andre Martins，文本机器学习简介：线性学习器，，PDF，<a
        href="http://lxmls.it.pt/2021/wp-content/uploads/2021/07/lxmls2021-andre-martins.pdf">原链接</a>, <a
        href="ppt/17-text/0_math.pdf">本地链接</a>

  <h4>重点 1：文本特征、分数的基本概念</h4>
  <ol>
    <li>第8-9页，示例</li>
    <ol>
      <li>文本分类中 P(类型|单词) 的统计及物理意义</li>
      <li>这些“单词”，通常被称作“特征”</li>
      <li>统计得到的 P(类型T|单词A) 表示了“单词A”对判断文档是否属于“类型T”的“绝对价值”，通常被称为“分数” - Score</li>
    </ol>
    <li>文本特征的表示</li>
    <ol>
      <li>特别简单，就是0/1</li>
      <li>出现了特征，比如单词“Love”，就标“1”，否则标“0”</li>
      <li>特征向量：比如一共有5个特征，可能表示为 [0,1,1,0,0]</li>
      <li>即：第1，4，5特征没有出现，2，3特征出现了</li>
    </ol>
    <li>文本特征的设计</li>
    <ol>
      <li>在上面用“单词”作为判断“文本类型”的“特征”</li>
      <li>除了单个单词，还可以有别的特征吗？</li>
      <li>可以，如: </li>
      <ol>
        <li>单词 + 上下文：n-gram，如 Jack London</li>
        <li>单词 + 单词属性：如 London （人名），这样就和 London（位置）区分开了</li>
        <li>单词 + 上下文 + 属性：如通过 Jack (人名）London，Go （动作）London ，我们可以区分这两个 London </li>
      </ol>
      <li>CRF就是采用了 “单词 + 上下文 + 属性” 的特征</li>
    </ol>
    <li>分数</li>
    <ol>
      <li>在上面通过统计得到 P(类型|单词)，作为“单词A”对判断文档是否属于“类型T”的“分数”</li>
      <li>除此之外，还可以有别的方法可以得到这个分数吗？</li>
      <li>可以，机器学习模型，将此作为一个最优化的问题，通过各种算法，得到最优“分数”</li>
    </ol>
    <li>什么是最优？</li>
    <ol>
      <li>最大熵</li>
      <li>最大似然估计</li>
      <li>最大后验概率</li>
      <li>Loss函数最小</li>
    </ol>
  </ol>

  <h4>重点 2：文本机器学习模型工作原理</h4>
  <ol>
    <li>需求</li>
    <ol>
      <li>设计一个模型，能够综合一个文档的各个特征 \phi(x_i)，得到其属于某一“类型y_i”的概率 P(y_i|X) </li>
      <li>然后各个类型的概率，选概率最大的，完成分类</li>
    </ol>
    <li>多元感知机模型</li>
    <ol>
      <li>最直观、简单、实用的一个模型，也是人工智能、深度学习模型的鼻祖</li>
      <li>模型：第58页 </li>
      <li>特点：简单线性模型</li>
      <li>训练方法：第60页</li>
      <li>原理：</li>
      <ol>
        <li>在线算法：来一个样本，就调一次“分数”</li>
        <li>分数在这里用w表示，即特征的权重（weight）</li>
        <li>如果样本类型应该是i，但被模型错误地分类为j，这意味着两个事情：
        <li>对类型i </li>
        <ol>
          <li>这意味着：这个样本的特征，在计算 P(y_i|X) 没有得到足够重视</li>
          <li>该样本的这些特征对应的 w 分数太低，需要增加</li>
          <li>怎么做？</li>
          <li>把文本的特征向量，加到 P(y_i|X) 模型的w上</li>
          <li>达到的效果：如果一个特征出现了（它的x是1），它的分数w就会被加1</li>
          <li>这样，这个样本的各个特征的权重在 P(y_i|X) 模型中是不是被提高了一些？</li>
          <li>这是我们想要的</li>
          <li>为什么只加1，不加100？</li>
          <li>不能着急，慢慢来。看算法，会继续迭代，下次发现错误了，还会加1，慢慢的就加上来了</li>
        </ol>
        <li>对类型j</li>
        <ol>
          <li>与上面对i的w的调整正好相反</li>
          <li>这意味着：这个样本的特征，在计算 P(y_j|X) 时，被错误地得到了太多的重视</li>
          <li>该样本的这些特征对应的 w 分数太高，需要减少</li>
          <li>怎么做？</li>
          <li>把文本的特征向量，在 P(y_j|X) 模型的w上减去</li>
          <li>达到的效果：如果一个特征出现了（它的x是1），它的分数w就会被减1</li>
          <li>这样，这个样本的各个特征在 P(y_j|X) 模型中权重是不是被减少了一些？</li>
          <li>这是我们想要的</li>
        </ol>
      </ol>
      <li>这就是第60页，多元感知机的内在逻辑</li>
      <li>理解这个对理解文本机器学习的特征工程和模型调整非常关键</li>
      <li>后面的所有模型，都是基于这个思路来的</li>
    </ol>
  </ol>

  <h4>重点 3：Naive Bayes 模型</h4>
  <ol>
    <li>两个特点：</li>
    <ol>
      <li>它比较的是 P(x,y)，不是上面提到的 P(Y|X) </li>
      <li>它通过条件独立假设，简化 P(X|Y) 的计算</li>
    </ol>
    <li>例：第71页-78页</li>
  </ol>

  <h4>重点 4：逻辑回归</h4>
  <ol>
    <li>特点</li>
    <ol>
      <li>第87页</li>
      <li>它用一个“指数家族”的形式，模型 P(y|x)</li>
      <li>请回顾数学基础部分“指数家族”概率分布的强大模型能力！</li>
      <li>它还是一个线性模型</li>
    </ol>
    <li>优化模型：第90页</li>
    <li>注意其中分两项：</li>
    <ol>
      <li>第一部分：Partition函数部分</li>
      <ol>
        <li>请回顾数学基础部分“指数家族”概率分布的Partition函数对\eta求导的公式</li>
      </ol>
      <li>第二部分：文本特征部分</li>
    </ol>
    <li>用梯度下降法求解</li>
    <ol>
      <li>第97页：核心的核心。一定要一步步跟下来</li>
      <li>关键是，对最后结果的理解：</li>
      <ol>
        <li>最后得到的梯度的第一部分，指的是：将当前样本的特征向量，乘以：模型得到的各种y的概率，作为这些y的模型的w的梯度</li>
        <li>最后得到的梯度的第二部分，指的是：将当前样本的特征向量，作为该样本的真实y的模型的w的梯度</li>
      </ol>
      <li>它的物理意义非常清楚</li>
      <li>和前面感知机的w的调整算法一脉相乘。非常有趣</li>
    </ol>
  </ol>

  <h4>实验</h4>
  <ol>
    <li>类似 Lab 5，完成 Day 1 的实验手册阅读和 linear_classifiers 目录下的1个实验。任务是亚马逊评论文本的情感分类，包括三个模型</li>
    <ol>
      <li>Naive Bayes</li>
      <li>Perceptron</li>
      <li>最大熵模型，其中又包括 L-BFGS、SGD 两种优化方法</li>
    </ol>
  </ol>

  <h3><a name="hmm">4、序列文本机器学习(HMM)</a></h3>

  <p>
    本节介绍大数据文本处理的序列模型：HMM 和 CRF
  </p>
  <p>材料</p>
  <ol>
    <li>Noah Smith，序列模型，PDF，<a href="ppt/17-text/2_seq.pdf">本地链接</a></li>
  </ol>

  <h4>重点 1：文本中为什么要”隐状态“？</h4>
  <ol>
    <li>我们需要感知单词的状态</li>
    <li>POS标签（词性）能够给单词加上更多有用的特征</li>
    <li>比如：“天”，可以是名词，也可以是感叹词。两者是完全不一样的。加了词性后，特征表示更加准确。</li>
    <li>NER（命名实体识别）对NLU（自然语言理解）非常重要</li>
  </ol>

  <h4>重点 2：维特比译码</h4>
  <ol>
    <li>根据POS标签、NER的有监督数据（有标签），能够很方便地统计出 HMM 的 两个概率</li>
    <li>数据量少的时候，加入先验概率，改进模型的泛化能力（第184页）</li>
    <ol>
      <li>Transition概率</li>
      <li>Emission概率</li>
    </ol>
    <li>这样就得到了 HMM 模型</li>
    <li>问题：来了一个新的句子，如何感知其单词的状态？</li>
    <li>算法：维特比译码</li>
    <li>原理 </li>
    <ol>
      <li>从前往后，对每个单词的每一种可能状态，都算出，从第一个单词开始，各种可能的状态序列下，到达该状态的，“最大”概率。直到最后一个单词</li>
      <li>最后那个单词的概率最大的状态，就是这个单词的状态的最佳估计</li>
      <li>然后从后往前，依次找这个最佳估计是从哪来的，就可以得到前一个单词的最佳状态</li>
      <li>由此完成所谓的“译码”</li>
    </ol>
    <li>观察它的式子，这是属于 max-product 的形式</li>
    <li>扩展：很多算法，都属于这种，叫 “动态规划”。它们原理差不多，只是形式不同，比如我们学过的 Dijkstra 最短路径算法</li>
  </ol>

  <h4>重点 3：句子出现概率</h4>
  <ol>
    <li>有了HMM模型，如何算出现在的这个句子的出现概率？</li>
    <li>类似维特比算法</li>
    <li>从前往后，对每个单词的每一种可能状态，都算出，从第一个单词开始，各种可能的状态序列下，到达该状态的，概率的“和”。直到最后一个单词</li>
    <ol>
      <li>和前面维特比算法比较，可以发现，它和维特比的唯一差别是：它计算“和”，维特比是计算“最大”</li>
      <li>这个叫“前向”算法</li>
      <li>最后一个单词的概率，就是这个句子出现的概率</li>
    </ol>
    <li>类似的，可以有“后向”算法</li>
    <ol>
      <li>从后往前，算出从这个状态出发，生成后面的单词的各种状态序列的概率的和。直到第一个单词</li>
      <li>第一个单词的概率，就是这个句子出现的概率</li>
    </ol>
    <li>观察它的式子，这是属于 add-product 的形式</li>
    <li>在抽象代数里，它们都属于“半环”的代数结构</li>
    <li>可以扩展到其它计算目标</li>
  </ol>

  <h4>重点 4：i位置单词的状态分布</h4>
  <ol>
    <li>精确地算出i位置单词的状态分布。利用它，就可以统计获得一个新的 Emission 概率，因此实现无监督HMM模型训练的一次迭代</li>
    <ol>
      <li>即经典的EM算法</li>
      <li>E：算i位置单词的状态分布（是平均的）</li>
      <li>M：基于状态分布，MLE估计，得到新的 Transition 和 Emission 概率</li>
    </ol>
    <li>方法</li>
    <ol>
      <li>固定 i位置单词的状态，为j，此时，生成当前句子的各种状态序列的总概率就是 i位置状态 为 j 的 前向概率 * 后向概率</li>
      <li>算出所有可能状态的这个概率，做归一化，就得到了 i位置单词的状态分布</li>
    </ol>
    <li>应用</li>
    <ol>
      <li>既然得到了i位置单词的状态分布，那么选择概率最大的状态，作为这个单词的状态，是不是就可以？</li>
      <li>可以，这就是“最大后验概率”解码</li>
      <li>它和维特比译码比起来，哪个好？</li>
      <li>要看你的评价指标，即成本函数</li>
      <ol>
        <li>如果是按句子级统计错误，即一个句子里错一个字，也算错，那么维特比译码好</li>
        <li>如果是按单词级统计错误，即一个句子里错一个字，就算一个错，那么这种译码好</li>
      </ol>
      <li>所以成本函数就很重要。你可以设计各种成本函数</li>
    </ol>
  </ol>

  <h4>重点 5：i位置单词的状态从j变为k的概率分布</h4>
  <ol>
    <li>利用它，就可以统计获得一个新的 Transition 概率，因此实现无监督HMM模型训练的一次迭代</li>
    <ol>
      <li>即经典的EM算法</li>
      <li>E：算i位置单词的状态分布（是平均的）</li>
      <li>M：基于状态分布，MLE估计，得到新的 Transition 和 Emission 概率</li>
    </ol>
    <li>方法</li>
    <ol>
      <li>固定 i位置单词的状态，为j，下一个位置的状态为k，此时，生成当前句子的各种状态序列的总概率就是 i位置状态 为 j 的 前向概率 * j的emission概率 * j到k的transition概率 * k的后向概率
      </li>
      <li>算出所有可能状态的这个概率，做归一化，就得到了 i位置单词状态 从 j变到k的分布</li>
    </ol>
  </ol>

  <h4>重点 6：EM算法</h4>
  <ol>
    <li>有两种</li>
    <ol>
      <li>软EM：如上</li>
      <li>硬EM：维特比译码后，基于译码的状态，计算 Transition 和 Emission 概率</li>
    </ol>
    <li>硬EM也挺好的</li>
    <li>EM是一种强大的通用方法。值得掌握</li>
  </ol>

  <h4>实验</h4>
  <ol>
    <li>类似 Lab 5，完成 Day 3 的实验手册阅读和 sequence_models 目录下的1个实验。任务是Conll的POS标签预测</li>
  </ol>

  <h3><a name="crf">5、文本结构化预测(CRF)</a></h3>

  <p>
    本节介绍结构化预测模型，特别是 CRF
  </p>
  <p>材料</p>
  <ol>
    <li>Xavier Carreras，学习结构化预测器，PDF，<a href="ppt/17-text/3_structure_pred.pdf">本地链接</a></li>
  </ol>

  <h4>重点 1：P(Y|X) 形式的序列模型</h4>
  <ol>
    <li>模型：第42页</li>
    <li>像 HMM，研究的是关于“序列”的模型</li>
    <li>和 HMM 不同之处是</li>
    <ol>
      <li>HMM 模型 P(X,Y)，是 “生成模型”</li>
      <li>CRF 的 P(Y|X) 是“辨别模型”</li>
    </ol>
    <li>我们学过的“辨别模型”包括：感知机，最大熵模型</li>
    <li>对分类问题，如 POS， NER，模型 P(Y|X) 更直接，效果可能更好</li>
  </ol>

  <h4>重点 2：结构化感知机</h4>
  <ol>
    <li>模型：第50页</li>
    <li>维特比译码，如果错误，像感知机那样，调整W</li>
    <li>通过平均，增加稳定性，能够大大提高模型性能</li>
    <li>具有感知机的各项优点</li>
    <ol>
      <li>在线算法</li>
      <li>几个回归就能得到好的结果</li>
      <li>效果接近更复杂的CRF算法</li>
      <li>可以通过Beam Search增加视界，改进性能</li>
    </ol>
  </ol>

  <h4>重点 3：Log-Linear序列预测模型（包括CRF）</h4>
  <ol>
    <li>模型：第56页</li>
    <li>扩展“指数家族”模型到序列预测</li>
    <li>Factored模型</li>
    <ol>
      <li>“特征”中包括 y_i 和 y_{i-1}</li>
      <li>就可以像HMM那样，用前向、后向、维特比算法（后面会看到）</li>
    </ol>
    <li>就变成一个优化问题，可以用梯度下降，优化求解w</li>
    <li>两种 Loss 函数</li>
    <ol>
      <li>MEMM：最大熵马尔科夫模型</li>
      <ol>
        <li>Log 似然，只考虑本地y_i就可以了（第60页梯度公式）</li>
        <li>“局部 Loss”</li>
      </ol>
      <li>CRF：</li>
      <ol>
        <li>Log 似然，考虑全部y序列（第61页）</li>
        <li>“全局 Loss“</li>
        <li>所以，要计算各种y序列的情况，然后求和（第62页梯度公式）</li>
        <li>因为 f 只取决于 i-1 和 i，所以可以把公式变为对 i-1 和 i 的各种情况求和（第63页）</li>
        <li>类似 HMM，用前向、后向算法，计算 y_{i-1} = a, y_{i} = b 的各种y序列的概率和 </li>
        <li>然后再对所有的 i 求和，这就得到了全局的”期望“特征向量（就是梯度里的第二项）</li>
        <li>梯度的第一项是将样本数据代入后的特征向量</li>
      </ol>
    </ol>
    <li>预测</li>
    <ol>
      <li>因为“特征”中包括 y_i 和 y_{i-1}，就可以用维特比译码方法，进行解码</li>
    </ol>
    <li>结合深度模型</li>
    <ol>
      <li>用深度模型得到深度表征，再进CRF（第77页）</li>
      <li>在CRF层还是这么调w，但同时也调下面的深度表征</li>
    </ol>
  </ol>

  <h4>实验</h4>
  <ol>
    <li>类似 Lab 5，完成 Day 4 的实验手册阅读和 learning_structured_predictors 目录下的4个实验。任务依然是Conll的POS标签预测，但包括如下算法</li>
    <ol>
      <li>CRF：ID特征</li>
      <li>CRF：扩展特征</li>
      <li>结构化感知机：ID特征</li>
      <li>结构化感知机：扩展特征</li>
    </ol>
  </ol>

  <h3><a name="nntxt">6、深度文本处理技术</a></h3>

  <p> 本节介绍深度文本处理技术</p>

  <h4> 内容 </h4>
  <ol>
    <li> 用 Log-Linear，MLP，RNN等模型进行文本分类和结构化预测</li>
    <li>首先用Numpy编写完成上述模型，因此真正懂得这些模型的原理和实现。这非常关键</li>
    <li>最后学习PyTorch框架中这些模型的使用</li>
  </ol>

  <h4>实验</h4>
  <ol>
    <li>类似 Lab 5，完成 Day 2 的实验手册阅读和 non_linear_classifiers 目录下的4个实验。任务是Amazon评论情感分类，包括
      <ol>
        <li>Log-Linear模型：Numpy版本</li>
        <li>Log-Linear模型：PyTorch版本</li>
        <li>MLP模型：Numpy版本</li>
        <li>MLP模型：PyTorch版本</li>
      </ol>
    <li>类似 Lab 5，完成 Day 5 的实验手册阅读和 non_linear_sequence_classifiers 目录下的4个实验。任务是WSJ的POS标签预测，包括
      <ol>
        <li>RNN模型：Numpy版本</li>
        <li>RNN模型：PyTorch版本</li>
      </ol>
  </ol>

  <h3><a name="intro">7、快速入门</a></h3>

  <p>1、王一行，《Python基础指南》，<a href="exercise/python.docx">Docx, 1.6MB</a></p>

  <p>2、张璇，《Python机器学习快速上手入门指南》，<a href="exercise/mllab.docx">Docx, 237KB</a>，<a href="exercise/mllab.pdf">PDF, 342KB</a>，Iris实验代码和数据，<a href="exercise/iris.zip">Zip，1.5MB</a></p>


    <!-- <h2 id="致谢">致谢</h2>
    <ul>
      <li></li>
    </ul> -->
  </body>
</html>
