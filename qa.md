# Map-Reduce

1. 大数据的两个特点是？
1. 一般来说，访问本机内存，本机硬盘，本机架上其它机器内存，其它机架上机器内存，吞吐量的比例是？
1. 大数据存储的特点是：它存的文件会经常更新，这一说法对吗？
1. 在Hadoop中，一个文件有多个备份，这一说法对吗？
1. 简述Namenode和DataNode的责任
1. 请画出Map-Reduce文件字数统计的原理图
1. Map-Reduce 中的 GroupbyKey 的功能是？它是由 系统环境实现的，还是由使用 Map-Reduce 开发应用的开发人员实现的？
1. GroupbyKey 会实现 Sort 吗？
1. 写出文本计数的Map-Reduce伪码
1. Map-Reduce 计算框架中，Master 是如何进行 Worker 的工作调度的？
1. 当Mapper 或 Reducer 节点失败时，系统如何进行处理？
1. 如何设置 Mapper 和 Reducer 的数目？
1. Map-Reduce 计算模型中只包括分布式存储，不包括计算，这句话对吗？为什么？
1. 用Map-Reduce实现矩阵和矩阵的乘法，如何设计Key？请说出原因。
1. 用Map-Reduce实现关系代数的Join，如何设计Key？请说出原因。
1. 关系代数中的 Projection，selection，join 如何用 Map-Reduce 实现？
1. Map-Reduce的开销模型主要考虑的是计算开销，这句话对吗？为什么？
1. 简述一种优化Map-Reduce开销的方法

# 机器学习

1. 回归和分类问题的区别是？根据email的内容，判断是否垃圾邮件，是回归还是分类？根据新闻内容，判断新闻类型，是回归还是分类？
1. 要评估一个机器学习模型的性能，就看它在训练集上的效果就好，这句话对吗？为什么？

# 感知机

1. 如果要根据email的内容，判断是否是垃圾邮件，如何构造特征向量？
1. 感知机模型的y，取值是1和0，还是1和-1，这样取值有什么好处？
1. 请写出或画出感知机模型的一般形式
1. 简述感知机模型的参数学习方法。它给Winnow算法带来什么启发？
1. 感知机收敛，需要样本空间符合什么特性？请画图说明
1. 写出Winnow算法的伪码
1. 感知机和Winnow算法，哪种是加法更新？哪种是乘法更新？如何用Map-Reduce实现它们？
1. 为什么说感知机算法是一种online算法？
1. 感知机算法的缺点是？如何克服？

# SVM

1. 简述SVM的基本原理
1. 如果x的维度为d，则SVM的支持向量一般有几个？
1. 写出Hinge Loss的数学表达式，画出它的图形
1. 写出SVM最小化的优化方程（包括目标函数和约束条件），指出其物理意义
1. 写出松弛（引入惩罚）后SVM最小化的的优化方程（包括目标函数和约束条件），指出其物理意义
1. 写出松弛后SVM的Loss函数，给出其导数的计算式
1. 给定3个训练的样本值，给出SVM的惩罚函数导数表
1. 简述基于惩罚函数导数表的SVM模型训练过程，请问如何用MapReduce实现它？
1. 简述SGD的原理、优缺点

# SVM 优化求解（可选）

1. 简述如何构造一个优化问题的拉格朗日函数
1. 简述为什么基于拉格朗日函数的极小极大问题，与原始优化问题等价
1. 写出基于拉格朗日函数的极小极大问题的对偶问题
1. 简述 KKT 条件
1. 写出线性可分SVM的优化问题的拉格朗日函数、原始极小极大问题、对偶问题。
1. 写出线性可分SVM的优化问题的对偶问题的求解过程
1. 写出完整的线性可分SVM的学习算法
1. 计算题，给定三个样本点，求解SVM

# 各种 ML

1. 简述KNN的基本原理
1. KNN的四要素是？
1. 简述什么是曼哈顿距离，什么是Jaccard距离？可以用示例说明
1. 在实际中，如何确定KNN的k？
1. KNN如何实现回归？如何做 Kernel 回归？
1. KNN是否适合高维数据？为什么？
1. 简述决策树的基本原理
1. 写出决策树的伪码。它的三个子模块分别是？一般如何实现这三个子模块？
1. 简述ID3，C4.5的基本原理
1. 如何防止决策树过拟合？
1. 简述随机森林的基本原理
1. 大数据的情况下，如何用MapReduce实现决策树？
1. 如果数据的维度很高（比如几万维），那么在SVM、KNN、决策树模型中，我们通常选用什么模型？为什么？
1. 什么是Ensemble方法？
1. Boost将很多非常强的模型组合到一起，这句话对吗？
1. 写出AdaBoost的伪码，简述其原理
1. 随着 base 模型数目的增长，AdaBoost 最后会过拟合，这句话对吗？为什么？
1. 为什么 AdaBoost 在训练集错误为0的情况下，继续训练，测试集的错误还可以减少？
1. 简述 Gradient Boosting 的原理
1. XGBoost 采用什么 base 模型作为 weak learner？

# Logistic 回归（LR）

1. 为什么说 Logistic 回归 是 Discriminative 分类器？ 写出其优化目标函数的数学表达式，然后解释
1. 写出 LR 模型的数学表达式，解释其物理意义
1. （案例分析题）给定一段文本，请设计对其进行情感分类的特征
1. 写出 0/1 分类问题 的 交叉熵 （Cross-Entropy）Loss 函数 的 数学表达式
1. 推导 LR 模型下， Cross-Entropy Loss 的 梯度。要求给出推导过程。给出推导结果的物理意义

# PCA 降维（可选）

1. 为什么要降维
1. 简述PCA降维的基本原理（可画图说明）
1. 简述将数据 X 投影到向量 v 后的方差计算方法
1. 简述 Spectral 定理。如何将该定理应用于 PCA 降维，获得各个主方向？
1. 如何对人脸数据做 PCA，得到的结果会是怎么样的？
1. 得到了特征向量后，如何进行投影？

# SVD 降维

1. 简述SVD降维的基本原理（可画图说明）
1. 什么是SVD降维的“概念空间”？如何将数据项（如用户）投影到概念空间？又如何利用用户的概念空间投影，推断其对物品（如电影）的兴趣？（可画图说明）
1. 如何用求特征向量矩阵的方法进行SVD分解？
1. 简述特征向量的一般计算方法，它的计算复杂度是？
1. 简述用Power Iteration方法求特征向量矩阵的基本原理。它的优点是？如何用大数据的方式实现？

# 推荐

1. 简述基于内容的推荐的基本原理
1. 简述TF-IDF的基本原理，写出公式
1. 简述Item-Item，User-User CF（Collaborative Filtering）的基本原理，写出对应的公式
1. 简述用SGD实现Item-Item CF推荐的基本原理，写出其目标函数和优化过程，给出每一个变量的物理意义
1. 简述用SGD实现基于矩阵UV分解的推荐的基本原理，写出其目标函数和优化过程，给出每一个变量的物理意义
1. 举例说明 Utility 矩阵的物理意义
1. 举例说明 Explicit rating 和 Implicit Rating 的 物理意义
1. 举例说明 “冷启动” 难题
1. 举例说明 基于内容的推荐 的 基本原理
1. （计算题）给定 电影 和 用户的 内容特征向量，计算其匹配度
1. 分析 基于内容的推荐 的 优缺点
1. 举例说明 基于 合作过滤 的推荐 的 基本原理
1. （计算题）给定 Utility 矩阵，计算一个用户对一个 Item 的可能评分
1. 实际中， 比较 基于 Item-Item 的 算法 和 基于 User-User 的算法，哪个 性能 好？为什么？
1. 分析 基于 合作过滤 的推荐 的 优缺点
1. 关于 Latent Factor 推荐模型
a）写出它的优化目标函数，解释其中各项的物理意义。
b）写出 通过 “梯度下降方法” 求解该优化问题的步骤。写出“梯度”的数学表达式。
c）写出 SGD 的英文全称，解释它和 GD 的差别，比较它们的优点

# 通过实验学习

1. “通过实验学习”的机器学习算法，其量化算法性能指标是什么？请写出其数学表达式，解释其物理意义。也请给出最优策略的数学形式，解释其物理意义。
1. 简述epsilon-greedy算法。它是否是最优策略？为什么？
1. UCB算法的英文全称是？简述UCB1算法的基本原理。它是否是最优策略？为什么？
1. 雅虎新闻推荐，考虑了哪些用户特征和文章特征，这些特性的选择有道理吗？
1. 雅虎新闻推荐，是如何将高维用户特征降维到文章目录上的？简述其原理
1. LinUCB算法中，如何计算Arm a的文章目录特征\sita_a的？然后又如何进行Arm选择的？写出它们的数学表达式，并解释其物理意义
1. 给出 MAB 问题的 最优策略 的 数学表达式，介绍其物理意义
1. 写出 Epsilon-Greedy 算法，分析其优点和不足。它是 最优策略 吗？
1. 写出 UCB 算法，分析其优点和不足。它是 最优策略 吗？
1. 举例说明，什么是 Contextual Bandit 问题

# 神经元网络（NN）

1. 画出 ReLU 激活函数的形状，给出其数学公式
1. 给出 Softmax 的数学表达式
1. 画图说明 NN 计算时的 Forward Pass 和 Backward Differentiation 的基本原理

# 深度表征

1. 简述有监督学习和无监督学习的区别
1. 给出 word2vec 算法的数学描述，解释其工作原理
1. 分析 word2vec 是有监督学习，还是无监督学习
1. 简述 word2vec 得到的表征的不足。为什么 Contextual representation (表征) 可以改进它？

# Transformer

1. 画图说明 Self-Attention 的基本原理，写出其数学形式。解释为什么需要它
1. 画图说明 Positional encoding 的基本原理，写出其数学形式。解释为什么需要它
1. 画图说明 多头注意力 Multi-Head Attention 的基本原理，写出其数学形式。解释为什么需要它
1. 画图说明 Masked attention 的基本原理，写出其数学形式。解释为什么需要它
1. 画出 Transfomer 的结构图，解释每一个模块的作用

# 语言模型（可选）

1. 给出语言模型的数学表达式，解释其物理意义
1. 给出 Unigram、Bigram、Trigram 语言模型的数学形式，解释其物理意义
1. （计算题）给定 Bigram 的 单词统计表，估计一句话 的 概率
1. 给出 Bigram 的 Perplexity 的 数学表达式，解释其物理意义
1. （是非题）在一个 数据集上 评估一个语言模型的性能时，其 Perplexity 越高越好
1. 给出 语言模型 估计时的 加1 Laplace 平滑 的数学表达式，解释其物理意义
1. 举例说明 Linear Interpolation 平滑 的基本原理，要求用数学公式解释
1. 举例说明 ”Stupid Backoff“ 平滑 的基本原理，要求用数学公式解释
1. 比较上述三种平滑方法的性能，说明其应用场合

# 预训练大语言模型

1. 简述采用 BERT 等 预训练模型 进行  Contextual representation (表征) 学习 的 原理
1. 简述 BERT 预训练 时 采用 的 MLM 方法
1. 画图说明，用 BERT 预训练模型 实现 文本分类 的 输入 和 输出 配置方法
1. 简述 如何 利用 BERT 预训练 模型 提取 文本 特征
1. 简述 GPT 和 BERT 的网络结构区别。结合该区别，解释 为什么 只有 GPT 可以做 文本生成

# Spark RDD

1. 为什么Spark比Hadoop快？
1. RDD的英文是？请解释其英文的每个单词的意义。
1. RDD的Partition起什么作用？它存储在内存里？磁盘里？还是内存，磁盘都有可能？
1. numRDD = sc.parallelize(["1","2","3"])语句的意义是？
1. RDD有两套函数：Transformation和Action。它们的区别是？
1. filter函数的作用是？它是Transformation，还是Action？
1. count函数的作用是？它是Transformation，还是Action？
1. collect函数的作用是？它是Transformation，还是Action？
1. take函数的作用是？它是Transformation，还是Action？
1. cache函数的作用是？
1. 用什么函数可以将一个操作应用于RDD的每个元素，并返回包含结果的新RDD？

1. 下面的代码完成什么工作？
假设输入数据文件中储存的是以下内容
    BJTU 200
    PKU 300
    Tsinghua 100
    BJTU 100
    PKU 200
    Tsinghua 20

那么，当它被下面的代码读入并运行后，将输出什么结果？

rdd = sc.textFile(...)
    .map(lambda s: s.split())
    .map(lambda x: (x[0], (float(x[1]), 1)))
    .reduceByKey(lambda t1, t2: (t1[0] + t2[0], t1[1] + t2[1]))
    .map(lambda t: (t[0], t[1][0] / t[1][1]))
    .collect()

# Spark DataFrame

1. 用DataFrame如何实现上述代码的功能？

1. 下面的代码有一处语法错误，请指出
    df2 = sqlContext.read.json("artists.json")
    df.join( df2,df.first_name = df2.firstName).show()

# 贝叶斯概率模型

1. 画出LDA模型的Plate符号图，解释每个参数的物理意义
1. 简述LDA模型的基本原理
1. 画出BPMF模型的Plate符号图，解释每个参数的物理意义
1. 简述BPMF模型的基本原理
