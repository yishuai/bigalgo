<!DOCTYPE html>
<html>
  <head>
    <title>机器学习算法：原理和应用</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  </head>
  <body>

    <h1>大数据存储和处理</h1>

    <h2>机器学习算法：原理和应用</h2>

    <p>陈一帅</p>

    <p><a href="mailto:yschen@bjtu.edu.cn">yschen@bjtu.edu.cn</a></p>

    <p>北京交通大学电子信息工程学院网络智能实验室</p>

    <img src="figure/cn.png" width="150" alt="2dmark" />

    <p>
      北京交通大学《大数据存储和应用》课程，源自斯坦福CS245大规模数据挖掘，讲解机器学习和数据挖掘算法的基本原理和算法，一路下来，带大家在动手中，走上算法研发的职业道路。详细课程信息请访问：https://yishuai.github.io/bigalgo
    </p>

    <h3>目录</h3>
    <ol>
      <li>大数据</li>
      <ul>
        <li><a href="#intro">大数据介绍</a></li>
        <li><a href="#memory">存储模型</a></li>
        <li><a href="#compute">计算模型</a></li>
      </ul>
      <li>Perceptron 感知机</li>
      <ul>
        <li><a href="#ml">机器学习基本概念</a></li>
        <li><a href="#perceptron">感知机</a></li>
        <li><a href="#perceptron-learn">感知机的学习</a></li>
        <li><a href="#perceptron-opti">感知机的优化</a></li>
        <li><a href="#opti">Winnow分类算法</a></li>
      </ul>
      <li>SVM 支持向量机</li>
      <ul>
        <li><a href="#svm">SVM支持向量机</a></li>
        <li><a href="#svm-learn">SVM的学习</a></li>
        <li><a href="#hinge">Hinge Loss</a></li>
        <li><a href="#loss">SVM Loss</a></li>
        <li><a href="#deriv">SVM梯度下降优化</a></li>
        <li><a href="#hinge-div">Hinge Loss导数表</a></li>
        <li><a href="#sgd">随机和Batch梯度下降</a></li>
      </ul>
      <li>贝叶斯模型</li>
      <ul>
        <li><a href="#bayesrule">贝叶斯推断</a></li>
        <li><a href="#condindep">条件独立</a></li>
        <li><a href="#textc">文本分类</a></li>
        <li><a href="#nbayes">朴素贝叶斯模型</a></li>
        <li><a href="#curse">维数诅咒</a></li>
        <li><a href="#skill">应用技巧</a></li>
        <li><a href="#bnet">贝叶斯网络</a></li>
        <li><a href="#marblanket">马尔科夫毯</a></li>
        <li><a href="mcmc">蒙特卡洛仿真</a></li>
        <li><a href="hyp">假设检验</a></li>
      </ul>
      <li>降维</li>
      <ul>
        <li><a href="#dimension">降维</a></li>
        <li><a href="#vector">特征值和特征向量</a></li>
        <li><a href="#pca">主元素分析</a></li>
        <li><a href="pcamath">主元素分析的数学理解</a></li>
        <li><a href="#svd">奇异值分解</a></li>
      </ul>
      <li>推荐</li>
      <ul>
        <li><a href="#recsys">推荐系统模型</a></li>
        <li><a href="#content">基于内容的推荐</a></li>
        <li><a href="#cf">协同过滤</a></li>
        <li><a href="#netflix">Netflix推荐大赛</a></li>
       </ul>
       <li>通过试验学习</li>
       <ul>
        <li><a href="#experi">通过试验学习</a></li>
        <li><a href="#mab">多臂老虎机模型</a></li>
        <li><a href="#regret">老虎机的悔恨</a></li>
        <li><a href="#explore">探索与利用</a></li>
        <li><a href="#epsilon">epsilon-贪心算法</a></li>
        <li><a href="#ucb">UCB 算法</a></li>
        <li><a href="#contextual">上下文老虎机</a></li>
        <li><a href="#linucb">LinUCB 算法</a></li>
       </ul>
       <li>文本智能</li>
       <ul>
        <li><a href="#text">文本处理</a></li>
        <li><a href="#lda">文本分析</a></li>
        <li><a href="#lm">语言模型</a></li>
        <li><a href="#pos-ner">词性标注与命名实体识别</a></li>
        <li><a href="#emb">向量语义与Embedding</a></li>
        <li><a href="#lr-nn">Logistic 回归 和 神经元网络</a></li>
        <li><a href="#transformer">Transformer</a></li>
        <li><a href="#pretrain">预训练模型</a></li>
        <li><a href="#dialog">对话系统</a></li>
        </ul>
        <li>文本算法</li>
        <ul>
        <li><a href="#algo-math">数学基础</a></li>
        <li><a href="#algo-opti">优化</a></li>
        <li><a href="#algo-classify">文本特征和分类</a></li>
        <li><a href="#hmm">序列文本学习(HMM)</a></li>
        <li><a href="#crf">结构化预测(CRF)</a></li>
        <li><a href="#nntxt">深度文本处理技术</a></li>
       </ul>
      <li>实验</li>
      <ul>
        <li><a href="#python">Python入门</a></li>
        <li><a href="#iris">机器学习编程入门</a></li>
        <li><a href="#chatbot">电影推荐对话机器人</a></li>
      </ul>
      <li>扩展</li>
      <ul>
        <li><a href="#ai">人工智能导论</a></li>
      </ul>
      </ol>

    <!-- == -->

    <h2>A、大数据</h2>

    <p>人类已经进入了大数据时代。数据就像空气、水、电力、能源一样，成为了最重要的生产要素。本章介绍大数据的特点和存储计算模型，为后面的大数据机器学习算法奠定基础。</p>

    <h3><a name="intro">一、大数据介绍</a></h3>

    <p>本节带大家了解大数据及其应用的特点，参观大数据中心</p>

    <p><a href="https://www.bilibili.com/video/BV1nZ4y1g7vt/">B站视频</a></p>
    <p>课程PPT：<a href="ppt/1-intro.pptx">PPT</a>（7MB）</p>

    <h3><a name="memory">二、存储模型</a></h3>

    <p>本节详细介绍大数据系统的存储模型和各项性能指标，然后学习目前最流行的分布式文件系统HDFS的基础知识。这是了解大数据系统的基础，对理解大数据系统性能至关重要。</p>

    <p><a href="https://www.bilibili.com/video/BV1SA411W7gf/">B站视频</a></p>

    <p>课程PPT：<a href="ppt/2-memory.pptx">PPT</a>（1MB）</p>

    <h3><a name="compute">三、计算模型</a></h3>

    <p>本节介绍Map-Reduce计算模型、框架、开销分析和优化。大数据计算就是通过Map-Reduce实现的，所以掌握这些内容非常重要。</p>

    <p><a href="https://www.bilibili.com/video/BV1e5411G7Qx/">B站视频</a></p>

    <p>课程PPT：<a href="ppt/3-mapreduce.pptx">PPT</a>（1MB）</p>

    <h2>B、感知机</h2>

    <p>感知机模型，从人类大脑神经元得到启发，具有完美几何解释，一手开创了人工智能。这样的模型，不了解行吗？不行。那就让我们开始吧。</p>

    <h3><a name="ml">四、机器学习基本概念</a></h3>

    <p>机器学习是从已知数据中学习出一个函数，然后用这个函数对未知的数据进行预测。本节我们简单了解一下这个概念。</p>

    <p><a href="https://www.bilibili.com/video/BV1v5411G7dR/">B站视频</a></p>
    <p>课程PPT：<a href="ppt/4-ml.pptx">PPT</a>（90KB）</p>

    <h3><a name="perceptron">五、感知机</a></h3>

    <p>感知机模型是一个非常优美、容易理解的机器学习模型。让我们以它为例子，理解什么是机器学习模型吧。很好理解的。试试吧？</p>

    <p><a href="https://www.bilibili.com/video/BV1xK41137kn/">B站视频</a></p>
    <p>课程PPT：<a href="ppt/5-perceptron.pptx">PPT</a>（1.8MB）</p>

    <h3><a name="rdd-lab">六、感知机的学习</a></h3>

    <p>感知机有着非常优美的几何描述。基于该几何描述，我们能够非常轻松地理解机器学习是如何从数据中学会一个模型的。这个过程非常有意思，就像人类一样，它能够从错误中改进自己，取得进步呢！所以犯错误真的是非常棒的，因为错误是最好的学习机会。</p>

    <p><a href="https://www.bilibili.com/video/BV1Ly4y1S7vp/">B站视频</a></p>
    <p>课程PPT：<a href="ppt/6-perceptron-model.pptx">PPT</a>（146KB）</p>

    <h3><a name="perceptron-opti">七、感知机的优化</a></h3>

    <p>感知机模型也有一些不足，比如它只能模型能够线性分隔的数据。这个缺点曾经导致感知机被放弃了很多年，直到深度学习挽救了它。本节我们介绍当数据线性不可分时，如何训练感知机模型，以及多元感知机和非线性感知机。它们让我们理解现实世界中的机器学习任务是非常复杂的，我们需要对数据有清楚的认识，才来训练出好的机器学习模型。这就是成为一个机器学习高手的秘诀。</p>

    <p><a href="https://www.bilibili.com/video/BV1c64y1f757/">B站视频</a></p>
    <p>课程PPT：<a href="ppt/7-perceptron-more.pptx">PPT</a>（168KB）</p>

    <h3><a name="opti">八、Winnow分类算法</a></h3>

    <p>Winnow分类算法和感知机很像，但它使用乘法。当许多维度无关时，它性能更好。它很简单，因此很适合高维数据，在大数据中很常用。</p>
    <p><a href="https://www.bilibili.com/video/BV1TV411h79P/">B站视频</a></p>
    <p>课程PPT：<a href="ppt/8-winnow.pptx">PPT</a>（129KB）</p>

    <h2>C、支持向量机</h2>

    <p> 具有最优美数学形式的支持向量机分类模型，自从被提出以来，就震惊了整个学术界。人们无法想象，这样美的模型怎么可能被人类发明，然而它确实被发明出来了。叹为观止。本章介绍支持向量机的原理和其梯度下降、随机梯度下降的优化方法。</p>

    <h3><a name="svm">九、SVM支持向量机</a></h3>

    <p>和感知机一样，SVM支持向量机也是要找到一个线性分隔平面。但它比感知机厉害。感知机只要训练集没有错误了，就停止优化了，而SVM还会继续优化，直到找到最佳的分隔平面为止。这是什么意思呢？</p>

    <p><a href="https://www.bilibili.com/video/BV1pi4y157Rk/">B站视频</a></p>
    <p>课程PPT：<a href="ppt/9-svm.pptx">PPT</a>（690KB）</p>

    <h3><a name="svm-learn">十、SVM的学习</a></h3>

    <p>本节介绍如何构建SVM的优化问题，找到最优线性分隔平面。这个过程非常有意思。</p>

    <p><a href="https://www.bilibili.com/video/BV1k54y167MX/">B站视频</a></p>
    <p>课程PPT：<a href="ppt/10-svm-model.pptx">PPT</a>（523KB）</p>

    <h3><a name="hinge">十一、Hinge Loss</a></h3>

    <p>加入Hinge Loss，对越过分隔平面的样本点进行惩罚，这让SVM更能容忍噪声，反映数据的本质特征。Hinge Loss非常有趣，让我们看看吧。</p>

    <p><a href="https://www.bilibili.com/video/BV1ja4y1H7J1/">B站视频</a></p>
    <p>课程PPT：<a href="ppt/11-svm-sgd.pptx">PPT</a>（776KB）</p>

    <h3><a name="loss">十二、SVM Loss</a></h3>

    <p>本节我们综合考虑分割平面的距离Loss和样本的Hinge Loss，得到整个SVM模型的Loss函数。通过控制该函数中的C参数，我们可以调节模型对噪声的容忍度，及其泛化能力。该Loss函数是Convex的，所以可以用梯度下降法优化，这就太方便了。</p>

    <p><a href="https://www.bilibili.com/video/BV1Wv411b7sE/">B站视频</a></p>

    <h3><a name="deriv">十三、SVM梯度下降优化</a></h3>

    <p>本节介绍如何计算SVM Loss函数的梯度，特别是Hinge Loss的梯度。得到了梯度后，我们就可以用梯度下降方法，从数据中学习SVM模型了！</p>

    <p><a href="https://www.bilibili.com/video/BV1p64y1f7FN/">B站视频</a></p>

    <h3><a name="hinge-deriv">十四、Hinge Loss导数表</a></h3>

    <p>本节介绍Hinge Loss导数表。我们将利用这个表，计算所有样本的Hinge Loss的导数。在大数据中，这个表会非常大，所以用Map-Reduce来实现它。了解这个对理解大数据下的SVM模型非常重要。让我们来看看吧。</p>

    <p><a href="https://www.bilibili.com/video/BV12a4y1W7NH/">B站视频</a></p>

    <h3><a name="sgd">十五、随机和Batch梯度下降</a></h3>

    <p>本节介绍随机梯度下降和Batch梯度下降方法的原理、实现和效果。这些方法能够极大地提高模型训练的速度（上万倍），所以是目前机器学习和深度学习中的主流方法，请一定好好理解它们。我们然后为你准备了一个斯坦福大学的SVM三种梯度下降方法的作业。请一定要完成它，这样你才会真正懂得梯度下降和SVM模型。记住，一定完成它！</p>

    <p><a href="https://www.bilibili.com/video/BV1sT4y1M7pV/">B站视频</a></p>

    <p>练习：<a href="hw/hw4.pdf">PDF</a>（274KB），<a href="hw/svm-hw.zip">Zip</a>（4.4MB）</p>

  <h2>D. 贝叶斯推断</h2>

    <p>伟大的贝叶斯定理，一直在人类探索世界的过程中处于绝对核心的位置，在此基础上，人们还提出了贝叶斯网络。它们都是我们探索世界的底层逻辑，做出最优决策的准绳。本章介绍它们及其在文本分类中的应用，即朴素贝叶斯分类器。 </p>

    <p><a href="ppt/12-bayes.pptx">PPT</a>（14.4MB）</p>

    <h3><a name="bayesrule">十六、贝叶斯推断</a></h3>

    <p>
      贝叶斯推断能基于收集到的证据，对特定假设的概率进行估计，比如“昨天是不是下雨了？”。它是统计机器学习的基石，因此是人工智能和机器学习的核心概念。本节通过讲解和举例，带大家理解贝叶斯推断的内涵。
    </p>

    <p><a href="https://www.bilibili.com/video/BV1H64y1f7wr/">B站视频</a></p>

    <h3><a name="condindep">十七、条件独立</a></h3>

    <p>本节介绍如何综合考虑多个证据，对特定假设的概率进行估计。为此，朴素贝叶斯分类器引入了条件独立。条件独立让贝叶斯分类变得简单、可扩展，性能还特别好。
    </p>

    <p><a href="https://www.bilibili.com/video/BV1Cy4y1v73H/">B站视频</a></p>

    <h3><a name="textc">十八、文本分类</a></h3>

    <p>朴素贝叶斯分类特别适合文本分类。本节通过示例，带大家完成自己的第一个贝叶斯文本分类器。这一方法非常实用，请一定要掌握哦。
    </p>

    <p><a href="https://www.bilibili.com/video/BV1gK4y1L7wu/">B站视频</a></p>

    <h3><a name="nbayes">十九、朴素贝叶斯模型</a></h3>

    <p>本节首先介绍朴素贝叶斯文本分类的数学模型，然后介绍机器学习的生成模型和判别模型基本概念，指出朴素贝叶斯模型是一个生成模型，这是它不同于感知机、支持向量机的地方。我们然后给出完整的朴素贝叶斯文本分类模型，包括对零概率的处理。这是我们第一次接触统计机器学习模型。模型的魅力是无穷的。
    </p>

    <p><a href="https://www.bilibili.com/video/BV1UA411W7C2/">B站视频</a></p>

    <h3><a name="curse">二十、维数诅咒</a></h3>

    <p>本节基于客户流失分类的例子，讲解我们在机器学习中经常遇到的一个非常重要的问题：维数诅咒，即：特征使用越多，数据越稀疏，导致分类器参数的精确估计变得更加困难。然后我们说明朴素贝叶斯是如何解决这个问题。这是一个理解维数诅咒的特别好的例子，
    </p>

    <p><a href="https://www.bilibili.com/video/BV1ia411F7iH/">B站视频</a></p>

    <h3><a name="skill">二十一、应用技巧</a></h3>

    <p>本节介绍在实际中运用朴素贝叶斯分类方法中可能遇到的两个问题：1）两种类别的先验概率极不平衡；2）连续变量，时的处理方法。这些方法在实际中非常有用。
    </p>

    <p><a href="https://www.bilibili.com/video/BV1ep4y1z7jG/">B站视频</a></p>

    <h3><a name="bnet">二十二、贝叶斯网络</a></h3>

    <p>贝叶斯网络能够将我们对世界的理解，特别是对各种关系的理解，引入机器学习模型。这个优点非常重要，因为我们特别希望我们的机器学习模型是能够解释，是符合我们理解的世界的规律的。我们前面学过的朴素贝叶斯分类器就是贝叶斯网络中的一种。本节通过讲解和举例，带大家理解并掌握贝叶斯网络。本节内容十分重要，请一定要掌握哦。
    </p>

    <p><a href="https://www.bilibili.com/video/BV17T4y1M7JJ/">B站视频</a></p>

    <h3><a name="marblanket">二十三、马尔科夫毯</a></h3>

    <p>本节分析几种贝叶斯网络中常见的元素关系的独立和条件独立，然后给出马尔科夫毯的概念。马尔科夫毯能帮助我们在一个贝叶斯网络中，定位和我们想要推断的元素的相关元素，因此展开测量和模型。本节内容十分重要。
    </p>

    <p><a href="https://www.bilibili.com/video/BV14T4y1M7Nt/">B站视频</a></p>

    <h3><a name="mcmc">二十三、蒙特卡洛仿真</a></h3>

    <p> 当理论分析难以进行时，我们还可以通过仿真的方法，研究概率问题。本节介绍蒙特卡洛仿真和马尔科夫链蒙特卡洛仿真。它们在实际中应用非常广泛，值得掌握。 </p>

    <p><a href="ppt/17-mcmc.pptx">PPT</a> (8.5M)</p>

    <h3><a name="hyp">二十三、假设检验原理</a></h3>

    <p> “提出假设”是数据分析的第一步。提出一个假设后，我们从数据中寻找相关证据，对该假设进行检验。本节介绍假设检验的基本原理，关于模型参数的假设检验，和非参数假设检验方法。</p>

    <p><a href="ppt/18-hypothesis.pptx">PPT</a> (367K)</p>

    <h2>E、降维</h2>

    <p>降维是机器学习改进性能的重要手段，同时隐含了重要的深度学习概念：表征学习，应用非常广泛。本章学习PCA（主元素分析）和SVD（奇异值分解）两种降维方法，非常有意思。</p>

    <p>降维原理：<a href="ppt/13-pca.pptx">PPT</a>（571KB）</p>

    <h3><a name="dimension">二十四、降维</a></h3>

    <p>机器学习是从数据中进行学习。如果数据包含冗余或无关变量，模型性能会下降。降维能够消除这些变量，提高模型性能。本节通过具体示例，解释为什么应该降维。
    </p>

    <p><a href="https://www.bilibili.com/video/BV1JA41147Mf/">B站视频</a></p>

    <h3><a name="vector">二十五、特征值和特征向量</a></h3>

    <p>本节介绍降维的数学基础：矩阵的特征值和特征向量。通过它们，我们就可以实现各种神奇的降维效果。本节特别介绍幂迭代（power iteration）计算特征向量的方法。该方法特别适合大数据场景。快来看看吧！
    </p>

    <p><a href="https://www.bilibili.com/video/BV1kX4y1M7yK/">B站视频</a></p>

    <h3><a name="pca">二十六、主元素分析</a></h3>

    <p>主元素分析（PCA）可以对原始输入数据进行变换，得到原始数据的正交基描述，而且输入数据在这些正交基上的能量还是依次递减的，第一个基就是数据的主元素。所以，我们就可以实现降维。当我们发现原始输入数据中的特征相关时，就应该做主元素分析。这非常重要，
    </p>

    <p><a href="https://www.bilibili.com/video/BV1ZT4y1T7R6/">B站视频</a></p>

    <h3><a name="pcamath">二十六、PCA的数学理解</a></h3>

    <p>
      PCA基于数据矩阵的协方差矩阵的特征分解，由此得到的特征值最大的特征向量指明了该数据的主方向，即数据在该方向上的投影的方差最大，依次类推还有其它次方向。这些方向相互正交。因此，将原始数据矩阵与这些主方向做乘积，就完成了向量投影，即坐标变换。这一过程非常有意思。本节从数学的角度，阐述PCA的基本原理。
    </p>

    <p><a href="ppt/13-pca-math.pptx ">PPT</a>（571KB）</p>

    <h3><a name="svd">二十七、奇异值分解</a></h3>

    <p>奇异值分解可以将一个矩阵分解为三个子矩阵的乘积，其中两个子矩阵分别反映了原始矩阵的行和列的基本信息，而另一矩阵的数值反映了它们在原始矩阵中的重要程度。基于它们，我们可以获得对样本和样本特征的降维描述，方便后续数据的处理和模型的学习，也可以据此进行推荐，非常有趣。本节我们练习斯坦福的另一个作业。一定要完成哦。
    </p>

    <p><a href="https://www.bilibili.com/video/BV14t4y1Y7VX/">B站视频</a></p>

    <p><a href="ppt/14-svd.pptx">PPT</a>（762KB）</p>

    <p>PCA和SVD练习：<a href="hw/hw2.pdf">PDF</a>（318KB）</p>

    <h2>F、推荐系统</h2>

    <p>哪个机器学习算法最值钱？推荐。我们现在买的大部分东西都不是我们主动寻找的，而是被推荐的。其它呢？读的书、观的影、看的文、交的友、走的路、爱的人、....，一切的一切、都是被推荐的。所以，本节我们学习推荐。</p>

    <p><a href="ppt/15-recsys.pptx">PPT</a>（3.9MB）</p>
    <p>参考：Dan Jurafsky, 推荐系统与合作过滤，<a href="http://web.stanford.edu/class/cs124/lec/collaborativefiltering21.pptx">PPT</a>（3MB）</p>

    <h3><a name="recsys">二十八、推荐系统模型</a></h3>

    <p>本节介绍推荐系统的基础模型：一个非常稀疏的矩阵描述。推荐系统的作用就是基于有限的数据样本，推断出用户可能最感兴趣的商品。
    </p>

    <p><a href="https://www.bilibili.com/video/BV1ra411F7yq/">B站视频</a></p>

    <h3><a name="content">二十九、基于内容的推荐</a></h3>

    <p>本节介绍如何利用TF-IDF等方法，提取、构建用户和商品的内容向量，然后匹配它们，为用户提供推荐，比如我们发现一篇文章是关于贝多芬的，而一位用户的历史表明他很喜欢贝多芬的文章，那么就为他推荐这篇文章。这就是基于内容的推荐。
    </p>

    <p><a href="https://www.bilibili.com/video/BV1gv411b7oo/">B站视频</a></p>

    <h3><a name="cf">三十、协同过滤</a></h3>

    <p>本节介绍另一种流行的推荐算法：协同过滤（CF：Collaborative Filter）。它通过用户-商品矩阵，发现相似用户或相似商品，进行推荐。该方法在实际中效果非常好。本节也简短地总结两种推荐系统，讨论推荐中常遇到的另一个问题：冷启动问题。
    </p>

    <p><a href="https://www.bilibili.com/video/BV1KV411h7tk/">B站视频</a></p>

    <h3><a name="netflix">三十一、Netflix推荐大赛</a></h3>

    <p>在推荐系统的发展史中，奖金高达100万美元的Netflix推荐大赛占据着史诗般的地位。正是这个大赛，极大地提高了推荐系统的知名度，让推荐系统研究成为显学。本节介绍该大赛的问题设置、基于SGD的协同过滤算法、矩阵分解方法、时变模型、集成方法，并回顾当年百万美元花落谁家的惊险一幕。
    </p>

    <p><a href="https://www.bilibili.com/video/BV1GK4y1L7Bm/">B站视频</a></p>

    <p>课本阅读
    <ol>
        <li>推荐系统英文阅读与书评，<a href="https://docs.qq.com/doc/DT010WHljRVhlVUhV">腾讯文档</a> </li>
      </ol>
    </p>

    <p>对话推荐系统练习
    <ol>
      <li>起始代码分析与运行，<a href="https://docs.qq.com/doc/DT1NpRUZFZXpkdW5M">腾讯文档</a> </li>
    </ol>
    </p>

    <p>算法练习
    <ol>
      <li>斯坦福 CS246 大数据处理练习：第3，4题，<a href="hw/hw2.pdf">PDF</a>（318KB），<a href="hw/hw2.zip">数据</a>（1.1MB）</li>
      <li>Lisben ML 夏令营 Python和梯度下降入门：<a href="https://docs.qq.com/doc/DT01ZaXpsck5IblJp">腾讯文档</a></li>
    </ol>
    </p>

    <h2>G. 通过试验学习</h2>

    <p>朋友，你有没有走进游戏厅，面对满屋老虎机，不知道如何下手？你知不知道，人生也有一点像老虎机，你需要在尝试中学习？你又想不想知道，要玩好人生老虎机，是要乐观呢，还是悲观？没错，这是一个机器学习问题。本节我们就来系统学习这个问题，找到老虎机的最优玩法。</p>

    <p><a href="ppt/16-bandit.pptx">PPT</a>（2.1MB）</p>

    <h3><a name="experi">三十二、通过试验学习</a></h3>

    <p>当你走进一个游戏厅，面对一排老虎机，是不是有一丝茫然：怎么玩才是最优的呢？这是一个通过实验进行学习的问题。本节介绍这一问题的基本概念和应用场景，你会发现原来它这么有用啊。
    </p>

    <p><a href="https://www.bilibili.com/video/BV1TZ4y137i6/">B站视频</a></p>

    <h3><a name="mab">三十三、多臂老虎机模型</a></h3>

    <p>本节介绍多臂老虎机（MAB）问题的模型。它是“通过实验学习”这一问题的经典模型。这个模型非常有趣，
    </p>

    <p><a href="https://www.bilibili.com/video/BV1qa4y1H7Vr/">B站视频</a></p>

    <h3><a name="regret">三十四、老虎机的悔恨</a></h3>

    <p>本节首先介绍多臂老虎机的性能评估指标：悔恨。这是通过试验学习特有的一个性能指标，非常有趣，也非常有道理。你一定会喜欢的。本节然后定义什么是多臂老虎机问题的最优策略。它非常深刻，很有意思，
    </p>

    <p><a href="https://www.bilibili.com/video/BV1wK411G7zP/">B站视频</a></p>

    <h3><a name="explore">三十五、探索与利用</a></h3>

    <p>
      通过实验学习的核心问题是：如何平衡“探索未知”和“利用已知”。这是决策制定中的经典难题。对这个问题的讨论，可能会给你带来极大的启发，改变你的人生！
    </p>

    <p><a href="https://www.bilibili.com/video/BV1y64y1f7j5/">B站视频</a></p>

    <h3><a name="epsilon">三十六、epsilon-贪心算法</a></h3>

    <p>本节介绍基于实验的学习的一个简单解法方法：epsilon-贪心算法。它会随着时间的增长，慢慢减少探索，增加利用。它可是一种最优算法哦：采用该算法，你最终会找到最优的老虎机。
    </p>

    <p><a href="https://www.bilibili.com/video/BV1o5411G7GK/">B站视频</a></p>

    <h3><a name="ucb">三十七、UCB 算法</a></h3>

    <p> UCB（置信区间上界）算法会选择置信区间上界最大的老虎机。这一方法既探索了未知，又利用了已知，让人拍案叫绝，成为目前应用最广泛的通过试验学习的算法。它也是一种最优算法：采用该算法，你最终会找到最优的老虎机。

    </p>

    <p><a href="https://www.bilibili.com/video/BV1Q54y1477k/">B站视频</a></p>

    <h3><a name="contextual">三十八、上下文老虎机</a></h3>

    <p> 上下文老虎机（Contextual Bandit）允许你在做决策时，考虑此时情境。比如在推荐新闻时，考虑读者的喜好。这实在是太强大了，
    </p>

    <p><a href="https://www.bilibili.com/video/BV17K411G7Tr/">B站视频</a></p>

    <h3><a name="linucb">三十九、LinUCB 算法</a></h3>

    <p> 本节介绍经典的 Contextual Bandit 算法：LinUCB。它假设回报是情境变量的线性加权和，即线性回归模型。它会按UCB算法选出最好的老虎机，也会根据实验结果不断更新每个老虎机的回报模型。本节也介绍Yahoo是如何用该方法实现新闻推荐的，
    </p>

    <p><a href="https://www.bilibili.com/video/BV15z4y1r7Ag/">B站视频</a></a></p>

    <p>实验：基于微软 Vowpal Wabbit AI库 的 Contextual Bandit 实验
      <ul>
        <li>离线模型训练实验， <a href="https://vowpalwabbit.org/tutorials/contextual_bandits.html">介绍</a>，
          <a href="https://mybinder.org/v2/gh/VowpalWabbit/jupyter-notebooks/master?filepath=Contextual_bandits_and_Vowpal_Wabbit.ipynb"> 实验环境</a> </li>
        <li>个性化新闻推荐实验，<a href="https://vowpalwabbit.org/tutorials/cb_simulation.html">介绍</a>，
        <a href="https://mybinder.org/v2/gh/VowpalWabbit/jupyter-notebooks/master?filepath=Simulating_a_news_personalization_scenario_using_Contextual_Bandits.ipynb">实验环境</a>
        </li>
      </ul>
    </p>

    <h2>H. 文本智能</h2>

    <h3><a name="text">四十、文本处理</a></h3>

    <p> 本节介绍大数据文本处理的利器：
      <ul>
        <li>正则表达式</li>
        <li>命令行工具，包括 tr, grep, sort, uniq 等</li>
      </ul>
      它们会让你在实际的工作中如虎添翼！
    </p>

    <p><a href=""></a></a></p>
    <p>斯坦福 Dan Jurafsky 教授材料
      <ul>
        <li>文本处理，<a href="http://www.stanford.edu/class/cs124/lec/2_TextProc_Mar_25_2021.pptx">PPT</a>（3.6MB）</li>
        <li>常用命令行工具，<a href="http://www.stanford.edu/class/cs124/lec/124-2021-UnixForPoets.pptx">PPT</a>（400KB） </li>
        <li>SLP课本第2章：正则表达式、文本预处理、编辑距离，<a href="https://web.stanford.edu/~jurafsky/slp3/2.pdf">PDF</a></li>
      </ul>
    </p>

    <p>对话推荐系统练习
    <ol>
      <li>基本文本信息提取、用户情感感知、合作过滤，<a href="https://docs.qq.com/doc/DT3hydFFLelJWZGNX">腾讯文档</a> </li>
    </ol>
    </p>

    <h3><a name="lda">四十、文本分析</a></h3>
    <p>
      本节介绍大数据文本处理实用技术，包括文本处理流程，n-gram，TF-IDF，LDA的基本代码实验和范例代码。
    </p>

    <p>学习材料</p>
    <ol>
      <li>Rayid Ghani，芝加哥大学，Text Analytics 101，<a href="ppt/17-text/text_analytics_rayid.pdf">PDF</a>
      </li>
    </ol>

    <p>练习</p>
    <ol>
      <li>卡耐基梅隆大学，社会公益数据科学实验室，社会公益数据科学搭便车指南，第二课，文本分析部分，<a
        href="https://github.com/dssg/hitchhikers-guide/tree/master/sources/curriculum/2_data_exploration_and_analysis/text-analysis">Github代码</a>，文本特征提取，主题模型
      </li>
    </ol>

    <h3><a name="lm">四十一、语言模型</a></h3>

    <p>
      本节介绍大数据文本处理的基础模型：语言模型，包括：
      <ul>
        <li>链式规则，马尔科夫近似</li>
        <li>n-gram模型，最大似然估计，句子概率计算</li>
        <li>基于语言模型的文本困惑度评估和文本生成</li>
        <li>三种平滑方法及其适用场景</li>
        <ul>
          <li>加1-拉普拉斯平滑</li>
          <li>基于交织的Kneser-Ney方法</li>
          <li>适用于大数据的Stupid
          Backoff方法</li>
        </ul>
      </ul>
    </p>

    <p><a href=""></a></a></p>
    <p>斯坦福 Dan Jurafsky 教授材料
    <ul>
      <li>语言模型，<a href="http://www.stanford.edu/class/cs124/lec/lm2021.pptx">PPT</a>（3.9MB）</li>
      <li>SLP课本第3章：N-gram 语言模型，<a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">PDF</a></li>
      <li>演示网站：<a href="https://transformer.huggingface.co/doc/distil-gpt2">GPT2文本生成</a></li>
      <li>演示网站：<a href="https://gpt3demo.com/">基于GPT3的各种应用，如代码生成</a></li>
    </ul>
    </p>

    <h3><a name="pos-ner">四十二、词性标注与命名实体识别</a></h3>

    <p> 本节介绍词性标注（POS）与命名实体识别（NER）的基本概念。
    </p>

    <p><a href=""></a></a></p>
    <p>斯坦福 Dan Jurafsky 教授材料
    <ul>
      <li>词性标注与命名实体识别，<a href="http://www.stanford.edu/class/cs124/lec/8_POSNER_intro_May_6_2021.pptx">PPT</a>（2.5MB）</li>
      <li>SLP课本第8章：序列标签任务 POS 与 NER，<a href="https://web.stanford.edu/~jurafsky/slp3/8.pdf">PDF</a> </li>
      <li>图书：Introduction to Chinese Natural Language Processing，2010</li>
    </ul>
    </p>

    <h3><a name="emb">四十三、向量语义与Embedding</a></h3>

    <p> 本节从两个方面探索单词的意义：
    <p>语义学方面，介绍单词之间的关系，包括：</p>
    <ul>
      <li>同义</li>
      <li>反义</li>
      <li>相似</li>
      <li>相关</li>
      <li>含义</li>
    </ul>
    <p>向量语义：基于单词的语言学分布（如：经常一起出现的单词）来定义单词的意义，包括：</p>
    <ul>
      <li>TF-IDF（或 PMI）词向量：向量长，稀疏</li>
      <li>Word2vec 词向量：向量短，密</li>
      <ul>
        <li>实现方法：Skip-Gram，变成分类问题，类似 Logistic Regression 建模，梯度下降法优化</li>
        <li>结果和窗口长度有关：窗口小，单词表征体现两个单词的语法相似；窗口大，体现两个单词出现的相关</li>
        <li>注意：基于平行四边形方法的类比评估方法，仅对频繁词、短距离、少量关系有效，应用时应谨慎。</li>
      </ul>
    </ul>
    </p>

    <p><a href=""></a></a></p>
    <p>斯坦福 Dan Jurafsky 教授材料
    <ul>
      <li>向量语义与表征，<a href="http://www.stanford.edu/class/cs124/lec/week4_vectorsemantics2021.pptx">PPT</a>（2.5MB）</li>
      <li>SLP课本第6章：向量语义与表征，<a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf">PDF</a> </li>
    </ul>
    </p>

    <h3><a name="lr-nn">四十四、Logistic 回归 和 神经元网络</a></h3>

    <p> 本节介绍 Logistic 回归（LR）和 神经元网络（NN）的基本概念，包括：
      <ul>
        <li>Discriminative 概率分类模型 P(c|d) 的基本概念</li>
        <li>交叉熵 Loss 的由来</li>
        <li>LR 模型 交叉熵 Loss 对 参数 w 的梯度 的优美形式，即：（模型结果 - 真实值）* x，很容易实现 </li>
        <li>线性模型（如 感知机）对非线性数据的无能为力</li>
        <li>通过引入非线性激活函数，神经元网络对非线性数据的强大能力</li>
        <li>多元分类时的 Softmax 输出</li>
        <li>基于图的 NN 前向计算 和 后向梯度传播</li>
      </ul>
    </p>

    <p><a href=""></a></a></p>
    <p>斯坦福 Dan Jurafsky 教授材料
    <ul>
      <li>Logistic 回归，<a href="http://www.stanford.edu/class/cs124/lec/logisticregression2021.pptx">PPT</a>（14MB）</li>
      <li>SLP课本第5章：Logistic 回归，<a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf">PDF</a> </li>
      <li>神经元网络，<a href="http://www.stanford.edu/class/cs124/lec/7_NN.pptx">PPT</a>（15MB）</li>
      <li>SLP课本第7章：神经元网络，<a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf">PDF</a></li>
    </ul>
    </p>

    <p>对话推荐系统练习
    <ol>
      <li>Python机器学习编程基础，<a href="https://docs.qq.com/doc/DT2JmR21qTkNxekpu">腾讯文档</a></li>
      <li>Logistic 回归、神经元网络、用户需求和喜好感知, <a href="https://docs.qq.com/doc/DT1RHbVV4R1NqTVpt">腾讯文档</a></li>
    </ol>
    </p>

    <h3><a name="transformer">四十五、Transformer</a></h3>

    <p> 本节介绍 Attention、基于 Self Attention 的 序列模型Transformer。它们十分强大，是目前主流深度学习模型的核心模块。请一定要掌握它们。
    </p>

    <p><a href=""></a></a></p>
    <p>伯克利 Sergey Levine 教授材料
    <ul>
      <li>Seq2Seq 和 Attention 的基本概念，<a href="ppt/18-deep/1-seq2seq.pdf">PPT</a>（1.5MB）</li>
      <li>Transformers，<a href="ppt/18-deep/2-transformer.pdf">PDF</a> (1.4MB) </li>
    </ul>

    <p>练习
    <ol>
      <li>Transformer，<a href="https://docs.qq.com/doc/DT3VSdXV5UEVleGpz">腾讯文档</a></li>
    </ol>
    </p>

    <h3><a name="pretrain">四十五、预训练模型</a></h3>

    <p> 本节介绍 无监督 预训练 模型 的基本概念，包括 Word2Vec，预训练模型（ELMO、BERT、GPT）。它们十分强大，是目前主流深度学习模型的核心模块。请一定要掌握它们。
    </p>

    <p><a href=""></a></a></p>
    <p>学习材料
    <ul>
      <li>伯克利 Sergey Levine 教授，CS182，无监督预训练模型，<a href="ppt/18-deep/3-nlp.pdf">PPT</a>（2.2MB）</li>
      <li>斯坦福 John Hewitt，CS224N/Ling284，预训练模型，<a href="ppt/18-deep/3-cs224n-pretraining.pdf">PPT</a>（1.7MB）</li>
      <li>维也纳理工大学，Sebastian Hofstätter，Transformer、BERT 预训练模型 与 Huggingface Transformer，<a href="ppt/18-deep/3-tansformer-bert-pretraining.pdf">PPT</a>（1.2MB）</li>
    </ul>
    </p>

    <p>体验
      <ol>
        <li>Python Code，How to Fine Tune BERT for Text Classification using Transformers in Python, <a href="https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python">网页</a>，<a href="https://colab.research.google.com/drive/18Qqox_QxJkOs80XVYaoLsdum0dX-Ilxb">Colab</a> </li>
        <li>DialoGPT 对话实验，<a href="https://colab.research.google.com/drive/1KAg6X8RFHE0KSvFSZ__w7KGZrSqT4cZ3">Colab</a></li>
      </ol>
    </p>

    <p>练习
      <ol>
        <li>预训练模型，<a href="https://docs.qq.com/doc/DT0V5Z21yRHpwTEpx">腾讯文档</a></li>
    </ol>
    </p>

    <h3><a name="dialog">四十六、对话系统</a></h3>

    <p> 本节介绍对话系统的基本概念，包括：
    <ul>
      <li>Chatbot 和 面向任务的对话客服 的基本概念</li>
      <li>ELIZA 对话机器人原理</li>
      <li>基于 Frame 的面向任务对话系统 </li>
      <li>人类对话的结构化属性</li>
      <li>对话中 Grounding 的重要意义</li>
      <li>多元分类时的 Softmax 输出</li>
      <li>意图分类 和 Slot 提取</li>
      <li>对话状态跟踪、对话策略 和 响应生成 </li>
      <li>对话系统的评估</li>
      <li>对话系统设计方法</li>
    </ul>
    </p>

    <p><a href=""></a></a></p>
    <p>斯坦福 Dan Jurafsky 教授材料
    <ul>
      <li>对话系统，<a href="http://www.stanford.edu/class/cs124/lec/24_Dialogue_May_6_2021.pptx">PPT</a>（17MB）</li>
      <li>SLP课本第24章：对话系统，<a href="https://web.stanford.edu/~jurafsky/slp3/24.pdf">PDF</a></li>
    </ul>
    </p>

    <p>对话推荐系统练习
    <ol>
      <li>面向任务的自然语言理解，<a href="https://docs.qq.com/doc/DT2FWSnR0c29uTFlx">腾讯文档</a></li>
    </ol>
    </p>

    <h2>G. 文本算法</h2>

    <p> LxMLS 2021 夏令营材料</p>
    <ol>
      <li>夏令营：<a href="http://lxmls.it.pt/2021/?page_id=79">主页</a></li>
      <li>课程视频：<a href="https://www.youtube.com/c/LxMLSLisbonMachineLearningSchool/">Youtube</a></li>
      <li>实验手册：<a href="http://lxmls.it.pt/2021/LxMLS_guide_2021.pdf">PDF</a></li>
      <li>实验代码：<a href="https://github.com/LxMLS/lxmls-toolkit">Github</a></li>
      <li>重点提示：<a href="ppt/17-text/lisben-index.pdf">PDF</a></li>
    </ol>

    <p>说明</p>
    <ol>
      <li>将代码和模型相印证，是学习机器学习和深度学习的最佳方法</li>
      <li>结合上课内容，搞懂模型是怎么代码实现的，直到自己能写出来，才真正理解和掌握了这些算法，并具备了机器学习的研究和开发能力</li>
      <li>这套代码就是一套帮助你掌握这些算法的最佳工具</li>
      <li>这些代码非常清晰，易懂。某种程度上，比看公式推导，更容易懂</li>
      <li>目标是自己能从0开始，把算法写出来，并把实验完成</li>
      <li>这是真正值得学习的</li>
    </ol>

    <h3><a name="algo-math">G.1 数学基础</a></h3>

    <p>
      本节介绍大数据算法的数学基础
    </p>
      <p>材料</p>
      <ol>
        <li>Mario Figueiredo，概率论和线性代数基础教程，PDF，<a
            href="http://lxmls.it.pt/2021/wp-content/uploads/2021/07/LxMLS_Lecture_0_handout.pdf">原链接</a>, <a
            href="ppt/17-text/0_math.pdf">本地链接</a></li>
      </ol>
      <p>重点</p>
      <ol>
        <li>Binomial分布，Multinomial分布</li>
        <li>高斯分布</li>
        <li>贝叶斯定理</li>
        <li>指数族：重点的重点。第36页-第38页详细推导</li>
        <li>特别是“指数家族”概率分布的Partition函数对\eta求导过程。这对理解后面的所有模型（直到CRF）都非常关键</li>
        <li>向量空间、范数、内积</li>
        <li>高斯-施瓦茨不等式，特别是第59页其精妙的证明</li>
        <li>矩阵的Rank、SVD</li>
        <li>Convex函数</li>
      </ol>

  <h3><a name="algo-opti">G.2 优化入门</a></h3>

  <p>
    本节介绍大数据算法的编程基础，包括Python入门、梯度下降、线性回归
  </p>
  <p>材料</p>
  <ol>
    <li>Luis Pedro Coelho, Python、Numpy、Matplotlib 入门，<a
        href="https://github.com/luispedro/talk-python-intro">Github</a></li>
    <li>Andre Martins，文本机器学习简介：线性学习器，PDF，<a
        href="http://lxmls.it.pt/2021/wp-content/uploads/2021/07/lxmls2021-andre-martins.pdf">原链接</a>, <a
        href="ppt/17-text/0_math.pdf">本地链接</a></li>
  </ol>
  <p>重点</p>
  <ol>
    <li>第26-34页，线性回归模型</li>
    <ol>
      <li>扩展，支持非线性</li>
      <li>误差函数概念：Squared Loss</li>
      <li>概率理解：基于高斯噪声和最大似然估计，基于高斯先验的L2正则</li>
    </ol>
    <li>第92页-93页，梯度下降优化方法</li>
  </ol>
  <p>实验：Lab5：父子身高相关吗？<a href="https://docs.qq.com/doc/DT1hkcUZZR2RvVktG">腾讯文档</a></p>

  <h3><a name="algo-classify">G.3 文本特征和分类模型</a></h3>

  <p>
    本节介绍大数据文本处理的特征工程和分类模型基本概念。
  </p>
  <p>材料：Andre Martins，文本机器学习简介：线性学习器，，PDF，<a
        href="http://lxmls.it.pt/2021/wp-content/uploads/2021/07/lxmls2021-andre-martins.pdf">原链接</a>, <a
        href="ppt/17-text/0_math.pdf">本地链接</a>

  <h4>重点 1：文本特征、分数的基本概念</h4>
  <ol>
    <li>第8-9页，示例</li>
    <ol>
      <li>文本分类中 P(类型|单词) 的统计及物理意义</li>
      <li>这些“单词”，通常被称作“特征”</li>
      <li>统计得到的 P(类型T|单词A) 表示了“单词A”对判断文档是否属于“类型T”的“绝对价值”，通常被称为“分数” - Score</li>
    </ol>
    <li>文本特征的表示</li>
    <ol>
      <li>特别简单，就是0/1</li>
      <li>出现了特征，比如单词“Love”，就标“1”，否则标“0”</li>
      <li>特征向量：比如一共有5个特征，可能表示为 [0,1,1,0,0]</li>
      <li>即：第1，4，5特征没有出现，2，3特征出现了</li>
    </ol>
    <li>文本特征的设计</li>
    <ol>
      <li>在上面用“单词”作为判断“文本类型”的“特征”</li>
      <li>除了单个单词，还可以有别的特征吗？</li>
      <li>可以，如: </li>
      <ol>
        <li>单词 + 上下文：n-gram，如 Jack London</li>
        <li>单词 + 单词属性：如 London （人名），这样就和 London（位置）区分开了</li>
        <li>单词 + 上下文 + 属性：如通过 Jack (人名）London，Go （动作）London ，我们可以区分这两个 London </li>
      </ol>
      <li>CRF就是采用了 “单词 + 上下文 + 属性” 的特征</li>
    </ol>
    <li>分数</li>
    <ol>
      <li>在上面通过统计得到 P(类型|单词)，作为“单词A”对判断文档是否属于“类型T”的“分数”</li>
      <li>除此之外，还可以有别的方法可以得到这个分数吗？</li>
      <li>可以，机器学习模型，将此作为一个最优化的问题，通过各种算法，得到最优“分数”</li>
    </ol>
    <li>什么是最优？</li>
    <ol>
      <li>最大熵</li>
      <li>最大似然估计</li>
      <li>最大后验概率</li>
      <li>Loss函数最小</li>
    </ol>
  </ol>

  <h4>重点 2：文本机器学习模型工作原理</h4>
  <ol>
    <li>需求</li>
    <ol>
      <li>设计一个模型，能够综合一个文档的各个特征 \phi(x_i)，得到其属于某一“类型y_i”的概率 P(y_i|X) </li>
      <li>然后各个类型的概率，选概率最大的，完成分类</li>
    </ol>
    <li>多元感知机模型</li>
    <ol>
      <li>最直观、简单、实用的一个模型，也是人工智能、深度学习模型的鼻祖</li>
      <li>模型：第58页 </li>
      <li>特点：简单线性模型</li>
      <li>训练方法：第60页</li>
      <li>原理：</li>
      <ol>
        <li>在线算法：来一个样本，就调一次“分数”</li>
        <li>分数在这里用w表示，即特征的权重（weight）</li>
        <li>如果样本类型应该是i，但被模型错误地分类为j，这意味着两个事情：
        <li>对类型i </li>
        <ol>
          <li>这意味着：这个样本的特征，在计算 P(y_i|X) 没有得到足够重视</li>
          <li>该样本的这些特征对应的 w 分数太低，需要增加</li>
          <li>怎么做？</li>
          <li>把文本的特征向量，加到 P(y_i|X) 模型的w上</li>
          <li>达到的效果：如果一个特征出现了（它的x是1），它的分数w就会被加1</li>
          <li>这样，这个样本的各个特征的权重在 P(y_i|X) 模型中是不是被提高了一些？</li>
          <li>这是我们想要的</li>
          <li>为什么只加1，不加100？</li>
          <li>不能着急，慢慢来。看算法，会继续迭代，下次发现错误了，还会加1，慢慢的就加上来了</li>
        </ol>
        <li>对类型j</li>
        <ol>
          <li>与上面对i的w的调整正好相反</li>
          <li>这意味着：这个样本的特征，在计算 P(y_j|X) 时，被错误地得到了太多的重视</li>
          <li>该样本的这些特征对应的 w 分数太高，需要减少</li>
          <li>怎么做？</li>
          <li>把文本的特征向量，在 P(y_j|X) 模型的w上减去</li>
          <li>达到的效果：如果一个特征出现了（它的x是1），它的分数w就会被减1</li>
          <li>这样，这个样本的各个特征在 P(y_j|X) 模型中权重是不是被减少了一些？</li>
          <li>这是我们想要的</li>
        </ol>
      </ol>
      <li>这就是第60页，多元感知机的内在逻辑</li>
      <li>理解这个对理解文本机器学习的特征工程和模型调整非常关键</li>
      <li>后面的所有模型，都是基于这个思路来的</li>
    </ol>
  </ol>

  <h4>重点 3：Naive Bayes 模型</h4>
  <ol>
    <li>两个特点：</li>
    <ol>
      <li>它比较的是 P(x,y)，不是上面提到的 P(Y|X) </li>
      <li>它通过条件独立假设，减化 P(X|Y) 的计算</li>
    </ol>
    <li>例：第71页-78页</li>
  </ol>

  <h4>重点 4：逻辑回归</h4>
  <ol>
    <li>特点</li>
    <ol>
      <li>第87页</li>
      <li>它用一个“指数家族”的形式，模型 P(y|x)</li>
      <li>请回顾数学基础部分“指数家族”概率分布的强大模型能力！</li>
      <li>它还是一个线性模型</li>
    </ol>
    <li>优化模型：第90页</li>
    <li>注意其中分两项：</li>
    <ol>
      <li>第一部分：Partition函数部分</li>
      <ol>
        <li>请回顾数学基础部分“指数家族”概率分布的Partition函数对\eta求导的公式</li>
      </ol>
      <li>第二部分：文本特征部分</li>
    </ol>
    <li>用梯度下降法求解</li>
    <ol>
      <li>第97页：核心的核心。一定要一步步跟下来</li>
      <li>关键是，对最后结果的理解：</li>
      <ol>
        <li>最后得到的梯度的第一部分，指的是：将当前样本的特征向量，乘以：模型得到的各种y的概率，作为这些y的模型的w的梯度</li>
        <li>最后得到的梯度的第二部分，指的是：将当前样本的特征向量，作为该样本的真实y的模型的w的梯度</li>
      </ol>
      <li>它的物理意义非常清楚</li>
      <li>和前面感知机的w的调整算法一脉相乘。非常有趣</li>
    </ol>
  </ol>

  <h4>实验</h4>
  <ol>
    <li>类似 Lab 5，完成 Day 1 的实验手册阅读和 linear_classifiers 目录下的1个实验。任务是亚马逊评论文本的情感分类，包括三个模型</li>
    <ol>
      <li>Naive Bayes</li>
      <li>Perceptron</li>
      <li>最大熵模型，其中又包括 L-BFGS、SGD 两种优化方法</li>
    </ol>
  </ol>

  <h3><a name="hmm">G.4 序列文本机器学习(HMM)</a></h3>

  <p>
    本节介绍大数据文本处理的序列模型：HMM 和 CRF
  </p>
  <p>材料</p>
  <ol>
    <li>Noah Smith，序列模型，PDF，<a href="ppt/17-text/2_seq.pdf">本地链接</a></li>
  </ol>

  <h4>重点 1：文本中为什么要”隐状态“？</h4>
  <ol>
    <li>我们需要感知单词的状态</li>
    <li>POS标签（词性）能够给单词加上更多有用的特征</li>
    <li>比如：“天”，可以是名词，也可以是感叹词。两者是完全不一样的。加了词性后，特征表示更加准确。</li>
    <li>NER（命名实体识别）对NLU（自然语言理解）非常重要</li>
  </ol>

  <h4>重点 2：维特比译码</h4>
  <ol>
    <li>根据POS标签、NER的有监督数据（有标签），能够很方便地统计出 HMM 的 两个概率</li>
    <li>数据量少的时候，加入先验概率，改进模型的泛化能力（第184页）</li>
    <ol>
      <li>Transition概率</li>
      <li>Emission概率</li>
    </ol>
    <li>这样就得到了 HMM 模型</li>
    <li>问题：来了一个新的句子，如何感知其单词的状态？</li>
    <li>算法：维特比译码</li>
    <li>原理 </li>
    <ol>
      <li>从前往后，对每个单词的每一种可能状态，都算出，从第一个单词开始，各种可能的状态序列下，到达该状态的，“最大”概率。直到最后一个单词</li>
      <li>最后那个单词的概率最大的状态，就是这个单词的状态的最佳估计</li>
      <li>然后从后往前，依次找这个最佳估计是从哪来的，就可以得到前一个单词的最佳状态</li>
      <li>由此完成所谓的“译码”</li>
    </ol>
    <li>观察它的式子，这是属于 max-product 的形式</li>
    <li>扩展：很多算法，都属于这种，叫 “动态规划”。它们原理差不多，只是形式不同，比如我们学过的 Dijkstra 最短路径算法</li>
  </ol>

  <h4>重点 3：句子出现概率</h4>
  <ol>
    <li>有了HMM模型，如何算出现在的这个句子的出现概率？</li>
    <li>类似维特比算法</li>
    <li>从前往后，对每个单词的每一种可能状态，都算出，从第一个单词开始，各种可能的状态序列下，到达该状态的，概率的“和”。直到最后一个单词</li>
    <ol>
      <li>和前面维特比算法比较，可以发现，它和维特比的唯一差别是：它计算“和”，维特比是计算“最大”</li>
      <li>这个叫“前向”算法</li>
      <li>最后一个单词的概率，就是这个句子出现的概率</li>
    </ol>
    <li>类似的，可以有“后向”算法</li>
    <ol>
      <li>从后往前，算出从这个状态出发，生成后面的单词的各种状态序列的概率的和。直到第一个单词</li>
      <li>第一个单词的概率，就是这个句子出现的概率</li>
    </ol>
    <li>观察它的式子，这是属于 add-product 的形式</li>
    <li>在抽象代数里，它们都属于“半环”的代数结构</li>
    <li>可以扩展到其它计算目标</li>
  </ol>

  <h4>重点 4：i位置单词的状态分布</h4>
  <ol>
    <li>精确地算出i位置单词的状态分布。利用它，就可以统计获得一个新的 Emission 概率，因此实现无监督HMM模型训练的一次迭代</li>
    <ol>
      <li>即经典的EM算法</li>
      <li>E：算i位置单词的状态分布（是平均的）</li>
      <li>M：基于状态分布，MLE估计，得到新的 Transition 和 Emission 概率</li>
    </ol>
    <li>方法</li>
    <ol>
      <li>固定 i位置单词的状态，为j，此时，生成当前句子的各种状态序列的总概率就是 i位置状态 为 j 的 前向概率 * 后向概率</li>
      <li>算出所有可能状态的这个概率，做归一化，就得到了 i位置单词的状态分布</li>
    </ol>
    <li>应用</li>
    <ol>
      <li>既然得到了i位置单词的状态分布，那么选择概率最大的状态，作为这个单词的状态，是不是就可以？</li>
      <li>可以，这就是“最大后验概率”解码</li>
      <li>它和维特比译码比起来，哪个好？</li>
      <li>要看你的评价指标，即成本函数</li>
      <ol>
        <li>如果是按句子级统计错误，即一个句子里错一个字，也算错，那么维特比译码好</li>
        <li>如果是按单词级统计错误，即一个句子里错一个字，就算一个错，那么这种译码好</li>
      </ol>
      <li>所以成本函数就很重要。你可以设计各种成本函数</li>
    </ol>
  </ol>

  <h4>重点 5：i位置单词的状态从j变为k的概率分布</h4>
  <ol>
    <li>利用它，就可以统计获得一个新的 Transition 概率，因此实现无监督HMM模型训练的一次迭代</li>
    <ol>
      <li>即经典的EM算法</li>
      <li>E：算i位置单词的状态分布（是平均的）</li>
      <li>M：基于状态分布，MLE估计，得到新的 Transition 和 Emission 概率</li>
    </ol>
    <li>方法</li>
    <ol>
      <li>固定 i位置单词的状态，为j，下一个位置的状态为k，此时，生成当前句子的各种状态序列的总概率就是 i位置状态 为 j 的 前向概率 * j的emission概率 * j到k的transition概率 * k的后向概率
      </li>
      <li>算出所有可能状态的这个概率，做归一化，就得到了 i位置单词状态 从 j变到k的分布</li>
    </ol>
  </ol>

  <h4>重点 6：EM算法</h4>
  <ol>
    <li>有两种</li>
    <ol>
      <li>软EM：如上</li>
      <li>硬EM：维特比译码后，基于译码的状态，计算 Transition 和 Emission 概率</li>
    </ol>
    <li>硬EM也挺好的</li>
    <li>EM是一种强大的通用方法。值得掌握</li>
  </ol>

  <h4>实验</h4>
  <ol>
    <li>类似 Lab 5，完成 Day 3 的实验手册阅读和 sequence_models 目录下的1个实验。任务是Conll的POS标签预测</li>
  </ol>

  <h3><a name="crf">G.5 文本结构化预测(CRF)</a></h3>

  <p>
    本节介绍结构化预测模型，特别是 CRF
  </p>
  <p>材料</p>
  <ol>
    <li>Xavier Carreras，学习结构化预测器，PDF，<a href="ppt/17-text/3_structure_pred.pdf">本地链接</a></li>
  </ol>

  <h4>重点 1：P(Y|X) 形式的序列模型</h4>
  <ol>
    <li>模型：第42页</li>
    <li>像 HMM，研究的是关于“序列”的模型</li>
    <li>和 HMM 不同之处是</li>
    <ol>
      <li>HMM 模型 P(X,Y)，是 “生成模型”</li>
      <li>它是“辨别模型”</li>
    </ol>
    <li>我们学过的“辨别模型”包括：感知机，最大熵模型</li>
    <li>对分类问题，如 POS， NER，模型 P(Y|X) 更直接，效果可能更好</li>
  </ol>

  <h4>重点 2：结构化感知机</h4>
  <ol>
    <li>模型：第50页</li>
    <li>维特比译码，如果错误，像感知机那样，调整W</li>
    <li>通过平均，增加稳定性，能够大大提高模型性能</li>
    <li>具有感知机的各项优点</li>
    <ol>
      <li>在线算法</li>
      <li>几个回归就能得到好的结果</li>
      <li>效果接近更复杂的CRF算法</li>
      <li>可以通过Beam Search增加视界，改进性能</li>
    </ol>
  </ol>

  <h4>重点 3：Log-Linear序列预测模型（包括CRF）</h4>
  <ol>
    <li>模型：第56页</li>
    <li>扩展“指数家族”模型到序列预测</li>
    <li>Factored模型</li>
    <ol>
      <li>“特征”中包括 y_i 和 y_{i-1}</li>
      <li>就可以像HMM那样，用前向、后向、维特比算法（后面会看到）</li>
    </ol>
    <li>就变成一个优化问题，可以用梯度下降，优化求解w</li>
    <li>两种 Loss 函数</li>
    <ol>
      <li>MEMM：最大熵马尔科夫模型</li>
      <ol>
        <li>Log 似然，只考虑本地y_i就可以了（第60页梯度公式）</li>
        <li>“局部 Loss”</li>
      </ol>
      <li>CRF：</li>
      <ol>
        <li>Log 似然，考虑全部y序列（第61页）</li>
        <li>“全局 Loss“</li>
        <li>所以，要计算各种y序列的情况，然后求和（第62页梯度公式）</li>
        <li>因为 f 只取决于 i-1 和 i，所以可以把公式变为对 i-1 和 i 的各种情况求和（第63页）</li>
        <li>类似 HMM，用前向、后向算法，计算 y_{i-1} = a, y_{i} = b 的各种y序列的概率和 </li>
        <li>然后再对所有的 i 求和，这就得到了全局的”期望“特征向量（就是梯度里的第二项）</li>
        <li>梯度的第一项是将样本数据代入后的特征向量</li>
      </ol>
    </ol>
    <li>预测</li>
    <ol>
      <li>因为“特征”中包括 y_i 和 y_{i-1}，就可以用维特比译码方法，进行解码</li>
    </ol>
    <li>结合深度模型</li>
    <ol>
      <li>用深度模型得到深度表征，再进CRF（第77页）</li>
      <li>在CRF层还是这么调w，但同时也调下面的深度表征</li>
    </ol>
  </ol>

  <h4>实验</h4>
  <ol>
    <li>类似 Lab 5，完成 Day 4 的实验手册阅读和 learning_structured_predictors 目录下的4个实验。任务依然是Conll的POS标签预测，但包括如下算法</li>
    <ol>
      <li>CRF：ID特征</li>
      <li>CRF：扩展特征</li>
      <li>结构化感知机：ID特征</li>
      <li>结构化感知机：扩展特征</li>
    </ol>
  </ol>

  <h3><a name="nntxt">G.6 深度文本处理技术</a></h3>

  <p> 本节介绍深度文本处理技术</p>
  <h4> 内容 </h4>
  <ol>
    <li> 用 Log-Linear，MLP，RNN等模型进行文本分类和结构化预测</li>
    <li>首先用Numpy编写完成上述模型，因此真正懂得这些模型的原理和实现。这非常关键</li>
    <li>最后学习PyTorch框架中这些模型的使用</li>
  </ol>

  <h4>实验</h4>
  <ol>
    <li>类似 Lab 5，完成 Day 2 的实验手册阅读和 non_linear_classifiers 目录下的4个实验。任务是Amazon评论情感分类，包括
      <ol>
        <li>Log-Linear模型：Numpy版本</li>
        <li>Log-Linear模型：PyTorch版本</li>
        <li>MLP模型：Numpy版本</li>
        <li>MLP模型：PyTorch版本</li>
      </ol>
    <li>类似 Lab 5，完成 Day 5 的实验手册阅读和 non_linear_sequence_classifiers 目录下的4个实验。任务是WSJ的POS标签预测，包括
      <ol>
        <li>RNN模型：Numpy版本</li>
        <li>RNN模型：PyTorch版本</li>
      </ol>
  </ol>

    <h3><a name="end">结课</a></h3>

    <p>恭喜大家完成了大数据机器学习的内容。致敬。再会！
    </p>

    <p><a href="https://www.bilibili.com/video/BV1YV411h77D/">B站视频</a></p>

    <h2>J. 实验</h2>

    <h3><a name="python">A. Python入门</a></h3>

    <p>王一行，《Python基础指南》，<a href="exercise/python.docx">Docx, 1.6MB</a></p>

    <h3><a name="iris">B. 机器学习编程入门</a></h3>

    <p>张璇，《Python机器学习快速上手入门指南》，<a href="exercise/mllab.docx">Docx, 237KB</a>，<a href="exercise/mllab.pdf">PDF, 342KB</a>，Iris实验代码和数据，<a href="exercise/iris.zip">Zip，1.5MB</a></p>

    <h3><a name="chatbot">C. 电影推荐对话机器人</a></h3>

    <p>项目介绍与起始代码：斯坦福 CS124 电影推荐机器人，<a href="https://github.com/cs124/pa6-chatbot">Github</a></p>

    <p>实验报告
    <ol>
      <li>准备阶段</li>
      <ul>
        <li>推荐系统英文阅读与书评，<a href="https://docs.qq.com/doc/DT010WHljRVhlVUhV">腾讯文档</a> </li>
        <li>代码分析与运行，<a href="https://docs.qq.com/doc/DT1NpRUZFZXpkdW5M">腾讯文档</a> </li>
        <li>梯度下降优化，<a href="https://docs.qq.com/doc/DT01ZaXpsck5IblJp">腾讯文档</a>
        </li>
      </ul>
      <li>第一阶段：机器学习</li>
      <ul>
        <li>文本信息提取、用户喜好感知、和合作过滤推荐，<a href="https://docs.qq.com/doc/DT3hydFFLelJWZGNX">腾讯文档</a> </li>
        <li>Python机器学习编程基础，<a href="https://docs.qq.com/doc/DT2JmR21qTkNxekpu">腾讯文档</a></li>
        <li>基于 Logistic回归、神经元网络的用户需求和喜好感知, <a href="https://docs.qq.com/doc/DT1RHbVV4R1NqTVpt">腾讯文档</a></li>
      </ul>
      <li>第二阶段：深度学习</li>
      <ul>
        <li>Transformer，<a href="https://docs.qq.com/doc/DT3VSdXV5UEVleGpz">腾讯文档</a></li>
        <li>预训练模型，<a href="https://docs.qq.com/doc/DT0V5Z21yRHpwTEpx">腾讯文档</a></li>
      </ul>
      <li>第三阶段：对话系统设计</li>
      <ul>
        <li>面向任务的自然语言理解<a href="https://docs.qq.com/doc/DT2FWSnR0c29uTFlx">腾讯文档</a></li>
      </ul>
      <li>总结</li>
      <ul>
        <li>期末总结，<a href="hw/qa.md">Markdown文件</a></li>
      </ul>
    </ol>
    </p>

    <h2>H. 扩展</h2>

    <h3><a name="ai">A. AI入门</a></h3>

    <p>《人工智能导论》，<a href="exercise/ai.pdf">PDF，11MB</a></p>

    <!-- <h2 id="致谢">致谢</h2>
    <ul>
      <li></li>
    </ul> -->
  </body>
</html>
